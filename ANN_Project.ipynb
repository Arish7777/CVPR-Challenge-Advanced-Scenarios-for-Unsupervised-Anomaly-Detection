{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "773ddaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 07:35:18,821 - __main__ - INFO - Using device: cuda\n",
      "2025-05-12 07:35:18,826 - __main__ - INFO - Starting improved anomaly detection pipeline...\n",
      "2025-05-12 07:35:18,827 - __main__ - INFO - Processing category: bottle\n",
      "2025-05-12 07:35:18,828 - __main__ - INFO - Training dataset for category: bottle\n",
      "2025-05-12 07:35:18,829 - __main__ - INFO - Searching for good samples in: ./mvtec_anomaly_detection\\bottle\\train\\good\n",
      "2025-05-12 07:35:18,831 - __main__ - INFO - Found 209 training images.\n",
      "2025-05-12 07:35:18,832 - __main__ - INFO - Testing dataset for category: bottle\n",
      "2025-05-12 07:35:18,832 - __main__ - INFO - Searching for test good samples in: ./mvtec_anomaly_detection\\bottle\\test\\good\n",
      "2025-05-12 07:35:18,834 - __main__ - INFO - Found defect types for bottle: ['broken_large', 'broken_small', 'contamination']\n",
      "2025-05-12 07:35:18,835 - __main__ - INFO - Found 20 defective samples for broken_large\n",
      "2025-05-12 07:35:18,837 - __main__ - INFO - Found 22 defective samples for broken_small\n",
      "2025-05-12 07:35:18,839 - __main__ - INFO - Found 21 defective samples for contamination\n",
      "2025-05-12 07:35:31,924 - __main__ - INFO - Model device: cuda:0\n",
      "2025-05-12 07:35:31,925 - __main__ - INFO - Training model for category: bottle\n",
      "Epoch 1/10:   0%|          | 0/7 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 480\u001b[0m\n\u001b[0;32m    478\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GPUs!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    479\u001b[0m cudnn\u001b[38;5;241m.\u001b[39mbenchmark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 480\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 449\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    447\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    448\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model for category: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 449\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating model for category: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    451\u001b[0m metrics \u001b[38;5;241m=\u001b[39m evaluate_model(model, category, test_loader)\n",
      "Cell \u001b[1;32mIn[13], line 314\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, category, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[0;32m    312\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    313\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 314\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[0;32m    315\u001b[0m     images \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    316\u001b[0m     augmented_images \u001b[38;5;241m=\u001b[39m augmentation(images)\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[13], line 167\u001b[0m, in \u001b[0;36mMVTecDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    166\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths[idx]\n\u001b[1;32m--> 167\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransform must be provided!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    169\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\PIL\\Image.py:3431\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3431\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3432\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "# Suppress specific FutureWarning for GradScaler\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "class Config:\n",
    "    # Dataset parameters\n",
    "    dataset_path = \"./mvtec_anomaly_detection\"\n",
    "    categories = [\n",
    "        \"bottle\", \"cable\", \"capsule\", \"carpet\", \"grid\",\n",
    "        \"hazelnut\", \"leather\", \"metal_nut\", \"pill\", \"screw\",\n",
    "        \"tile\", \"toothbrush\", \"transistor\", \"wood\", \"zipper\"\n",
    "    ]\n",
    "    material_properties = {\n",
    "        \"bottle\": [\"transparent\", \"rigid\", \"container\", \"curved surface\"],\n",
    "        \"cable\": [\"flexible\", \"elongated\", \"connector\", \"insulated\"],\n",
    "        \"capsule\": [\"small\", \"cylindrical\", \"pharmaceutical\", \"colorful\"],\n",
    "        \"carpet\": [\"textured\", \"flat\", \"fabric\", \"patterned\"],\n",
    "        \"grid\": [\"regular pattern\", \"structured\", \"geometric\", \"repetitive\"],\n",
    "        \"hazelnut\": [\"organic\", \"natural\", \"edible\", \"oval shaped\"],\n",
    "        \"leather\": [\"textured\", \"flexible\", \"natural material\", \"durable\"],\n",
    "        \"metal_nut\": [\"metallic\", \"threaded\", \"hardware\", \"circular\"],\n",
    "        \"pill\": [\"medical\", \"small\", \"uniform\", \"pharmaceutical\"],\n",
    "        \"screw\": [\"metallic\", \"threaded\", \"hardware\", \"cylindrical\"],\n",
    "        \"tile\": [\"flat\", \"hard\", \"ceramic\", \"uniform\"],\n",
    "        \"toothbrush\": [\"plastic\", \"bristled\", \"handheld\", \"hygienic\"],\n",
    "        \"transistor\": [\"electronic\", \"semiconductor\", \"small\", \"technical\"],\n",
    "        \"wood\": [\"natural\", \"grain patterned\", \"organic\", \"fibrous\"],\n",
    "        \"zipper\": [\"metal\", \"plastic\", \"fastener\", \"interlocking\"]\n",
    "    }\n",
    "    generic_anomaly_types = [\n",
    "        \"scratched\", \"broken\", \"deformed\", \"contaminated\", \n",
    "        \"discolored\", \"cracked\", \"damaged\", \"misshapen\",\n",
    "        \"stained\", \"perforated\", \"corroded\", \"bent\"\n",
    "    ]\n",
    "    specific_anomaly_types = {\n",
    "        \"bottle\": [\"dented\", \"leaking\", \"chipped\", \"malformed neck\"],\n",
    "        \"cable\": [\"exposed wire\", \"frayed\", \"kinked\", \"connector damage\"],\n",
    "        \"capsule\": [\"crushed\", \"empty\", \"incomplete filling\", \"color variation\"],\n",
    "        \"carpet\": [\"worn\", \"torn\", \"faded\", \"stained\"],\n",
    "        \"grid\": [\"distorted\", \"misaligned\", \"broken lines\", \"irregular spacing\"],\n",
    "        \"hazelnut\": [\"moldy\", \"shriveled\", \"discolored\", \"pest damaged\"],\n",
    "        \"leather\": [\"scratched\", \"stained\", \"wrinkled\", \"uneven texture\"],\n",
    "        \"metal_nut\": [\"rusted\", \"cracked\", \"thread damage\", \"deformed\"],\n",
    "        \"pill\": [\"chipped\", \"split\", \"faded\", \"stained\"],\n",
    "        \"screw\": [\"stripped head\", \"bent\", \"rusted\", \"shortened\"],\n",
    "        \"tile\": [\"chipped\", \"cracked\", \"rough surface\", \"uneven color\"],\n",
    "        \"toothbrush\": [\"bent bristles\", \"missing bristles\", \"discolored\", \"malformed handle\"],\n",
    "        \"transistor\": [\"bent pins\", \"cracked casing\", \"missing components\", \"burn marks\"],\n",
    "        \"wood\": [\"splintered\", \"rotted\", \"knotted\", \"warped\"],\n",
    "        \"zipper\": [\"missing teeth\", \"detached\", \"stuck\", \"bent teeth\"]\n",
    "    }\n",
    "    target_size = (224, 224)  # Image size for model input\n",
    "    batch_size = 32\n",
    "    num_epochs = 10\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 1e-5\n",
    "    clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "    embedding_dim = 512\n",
    "    margin = 0.5\n",
    "    temperature = 0.07\n",
    "    anomaly_threshold = 0.5\n",
    "    vis_save_dir = \"visualization_results\"\n",
    "    checkpoint_dir = \"checkpoints\"\n",
    "    normal_prompt_templates = [\n",
    "        \"a photo of normal {category}\",\n",
    "        \"a normal {category} without defects\",\n",
    "        \"a pristine {category} in perfect condition\",\n",
    "        \"a flawless {category} with no anomalies\"\n",
    "    ]\n",
    "    anomaly_prompt_templates = [\n",
    "        \"a photo of {category} with {anomaly_type}\",\n",
    "        \"a defective {category} showing {anomaly_type}\",\n",
    "        \"a {category} that has {anomaly_type} areas\",\n",
    "        \"a damaged {category} with visible {anomaly_type}\"\n",
    "    ]\n",
    "    material_prompt_templates = [\n",
    "        \"a {material_property} {category} in normal condition\",\n",
    "        \"a {material_property} {category} without defects\"\n",
    "    ]\n",
    "    material_anomaly_templates = [\n",
    "        \"a {material_property} {category} with {anomaly_type}\",\n",
    "        \"a {material_property} {category} showing {anomaly_type}\"\n",
    "    ]\n",
    "    use_fp16 = True  # Use mixed precision training\n",
    "    num_workers = 0  # Set to 0 for debugging DataLoader issues\n",
    "class MVTecDataset(Dataset):\n",
    "    def __init__(self, root_dir, category, is_train=True, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.category = category\n",
    "        self.is_train = is_train\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.mask_paths = []\n",
    "        self.defect_types = []\n",
    "        logger.info(f\"{'Training' if is_train else 'Testing'} dataset for category: {category}\")\n",
    "        if is_train:\n",
    "            train_good_path = os.path.join(root_dir, category, \"train\", \"good\")\n",
    "            logger.info(f\"Searching for good samples in: {train_good_path}\")\n",
    "            self.image_paths = sorted(glob.glob(os.path.join(train_good_path, \"*.png\")))\n",
    "            logger.info(f\"Found {len(self.image_paths)} training images.\")\n",
    "            if len(self.image_paths) == 0:\n",
    "                raise ValueError(f\"No training images found for category '{category}'\")\n",
    "            self.labels = [0] * len(self.image_paths)\n",
    "            self.mask_paths = [None] * len(self.image_paths)\n",
    "            self.defect_types = [\"none\"] * len(self.image_paths)\n",
    "        else:\n",
    "            test_good_path = os.path.join(root_dir, category, \"test\", \"good\")\n",
    "            logger.info(f\"Searching for test good samples in: {test_good_path}\")\n",
    "            good_paths = sorted(glob.glob(os.path.join(test_good_path, \"*.png\")))\n",
    "            self.image_paths.extend(good_paths)\n",
    "            self.labels.extend([0] * len(good_paths))\n",
    "            self.defect_types.extend([\"none\"] * len(good_paths))\n",
    "            defect_types = [\n",
    "                d for d in os.listdir(os.path.join(root_dir, category, \"test\"))\n",
    "                if d != \"good\" and os.path.isdir(os.path.join(root_dir, category, \"test\", d))\n",
    "            ]\n",
    "            logger.info(f\"Found defect types for {category}: {defect_types}\")\n",
    "            for defect_type in defect_types:\n",
    "                defect_path = os.path.join(root_dir, category, \"test\", defect_type)\n",
    "                defect_paths = sorted(glob.glob(os.path.join(defect_path, \"*.png\")))\n",
    "                logger.info(f\"Found {len(defect_paths)} defective samples for {defect_type}\")\n",
    "                self.image_paths.extend(defect_paths)\n",
    "                self.labels.extend([1] * len(defect_paths))\n",
    "                self.defect_types.extend([defect_type] * len(defect_paths))\n",
    "                for img_path in defect_paths:\n",
    "                    img_filename = os.path.basename(img_path)\n",
    "                    mask_path = os.path.join(\n",
    "                        root_dir, category, \"ground_truth\", defect_type, img_filename\n",
    "                    )\n",
    "                    mask_with_suffix = os.path.splitext(mask_path)[0] + \"_mask.png\"\n",
    "                    if os.path.exists(mask_path):\n",
    "                        self.mask_paths.append(mask_path)\n",
    "                    elif os.path.exists(mask_with_suffix):\n",
    "                        self.mask_paths.append(mask_with_suffix)\n",
    "                    else:\n",
    "                        self.mask_paths.append(None)\n",
    "                        logger.warning(f\"No mask found for {img_path}\")\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image_path = self.image_paths[idx]\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            assert self.transform is not None, \"Transform must be provided!\"\n",
    "            image = self.transform(image)\n",
    "            label = self.labels[idx]\n",
    "            defect_type = self.defect_types[idx]\n",
    "            mask_path = self.mask_paths[idx]\n",
    "            mask = None\n",
    "            if mask_path and os.path.exists(mask_path):\n",
    "                mask = Image.open(mask_path).convert(\"L\")\n",
    "                mask = transforms.ToTensor()(mask)\n",
    "            else:\n",
    "                mask = torch.zeros((1, *Config.target_size))  # Placeholder\n",
    "            return {\n",
    "                \"image\": image,\n",
    "                \"label\": label,\n",
    "                \"mask\": mask,\n",
    "                \"image_path\": image_path,\n",
    "                \"defect_type\": defect_type\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading sample {idx} from {self.image_paths[idx]}: {str(e)}\")\n",
    "            raise\n",
    "class PromptBasedAnomalyDetector(nn.Module):\n",
    "    def __init__(self, clip_model_name=\"openai/clip-vit-base-patch32\"):\n",
    "        super(PromptBasedAnomalyDetector, self).__init__()\n",
    "        self.clip_model = CLIPModel.from_pretrained(clip_model_name)\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(clip_model_name)\n",
    "        self.text_encoder = self.clip_model.text_model\n",
    "        self.vision_encoder = self.clip_model.vision_model\n",
    "        self.text_proj = self.clip_model.text_projection\n",
    "        self.vision_proj = self.clip_model.visual_projection\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        num_normal_templates = len(Config.normal_prompt_templates)\n",
    "        num_anomaly_templates = len(Config.anomaly_prompt_templates)\n",
    "        num_material_templates = len(Config.material_prompt_templates)\n",
    "        num_material_anomaly_templates = len(Config.material_anomaly_templates)\n",
    "        self.normal_prompt_embeddings = nn.Parameter(\n",
    "            torch.randn(num_normal_templates, self.text_encoder.config.hidden_size, device=device)\n",
    "        )\n",
    "        self.anomaly_prompt_embeddings = nn.Parameter(\n",
    "            torch.randn(num_anomaly_templates, self.text_encoder.config.hidden_size, device=device)\n",
    "        )\n",
    "        self.material_prompt_embeddings = nn.Parameter(\n",
    "            torch.randn(num_material_templates, self.text_encoder.config.hidden_size, device=device)\n",
    "        )\n",
    "        self.material_anomaly_embeddings = nn.Parameter(\n",
    "            torch.randn(num_material_anomaly_templates, self.text_encoder.config.hidden_size, device=device)\n",
    "        )\n",
    "        self.normal_weights = nn.Parameter(torch.ones(num_normal_templates, device=device))\n",
    "        self.anomaly_weights = nn.Parameter(torch.ones(num_anomaly_templates, device=device))\n",
    "        self.material_weights = nn.Parameter(torch.ones(num_material_templates, device=device))\n",
    "        self.material_anomaly_weights = nn.Parameter(torch.ones(num_material_anomaly_templates, device=device))\n",
    "        self.vit_layer = self.vision_encoder\n",
    "        self.anomaly_localization_head = nn.Sequential(\n",
    "            nn.Conv2d(self.vision_encoder.config.hidden_size, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 1, kernel_size=1)\n",
    "        ).to(device)\n",
    "    def encode_text(self, text):\n",
    "        text_inputs = self.tokenizer(text, padding=True, return_tensors=\"pt\").to(device)\n",
    "        text_outputs = self.text_encoder(**text_inputs)\n",
    "        text_embeddings = text_outputs.last_hidden_state[:, 0, :]\n",
    "        text_embeddings = self.text_proj(text_embeddings)\n",
    "        return text_embeddings\n",
    "    def encode_image(self, image):\n",
    "        vision_outputs = self.vision_encoder(image)\n",
    "        image_embeddings = vision_outputs.last_hidden_state[:, 0, :]\n",
    "        image_embeddings = self.vision_proj(image_embeddings)\n",
    "        patch_embeddings = vision_outputs.last_hidden_state[:, 1:, :]\n",
    "        batch_size = image.shape[0]\n",
    "        patch_size = 32\n",
    "        h = w = image.shape[2] // patch_size\n",
    "        patch_features = patch_embeddings.reshape(batch_size, h, w, -1).permute(0, 3, 1, 2)\n",
    "        return image_embeddings, patch_features\n",
    "    def get_category_specific_anomalies(self, category):\n",
    "        if category in Config.specific_anomaly_types:\n",
    "            return Config.specific_anomaly_types[category] + Config.generic_anomaly_types\n",
    "        return Config.generic_anomaly_types\n",
    "    def get_material_properties(self, category):\n",
    "        return Config.material_properties.get(category, [])\n",
    "    def generate_prompts(self, category_name):\n",
    "        normal_embeddings = []\n",
    "        normal_weights = F.softmax(self.normal_weights, dim=0)\n",
    "        for i, template in enumerate(Config.normal_prompt_templates):\n",
    "            prompt = template.format(category=category_name)\n",
    "            embedding = self.encode_text([prompt])\n",
    "            normal_embeddings.append(embedding + self.normal_prompt_embeddings[i])\n",
    "        material_weights = F.softmax(self.material_weights, dim=0)\n",
    "        for i, template in enumerate(Config.material_prompt_templates):\n",
    "            for prop in self.get_material_properties(category_name):\n",
    "                prompt = template.format(category=category_name, material_property=prop)\n",
    "                embedding = self.encode_text([prompt])\n",
    "                normal_embeddings.append(embedding + self.material_prompt_embeddings[i])\n",
    "        anomaly_embeddings = []\n",
    "        anomaly_weights = F.softmax(self.anomaly_weights, dim=0)\n",
    "        anomaly_types = self.get_category_specific_anomalies(category_name)\n",
    "        for i, template in enumerate(Config.anomaly_prompt_templates):\n",
    "            for anomaly_type in anomaly_types:\n",
    "                prompt = template.format(category=category_name, anomaly_type=anomaly_type)\n",
    "                embedding = self.encode_text([prompt])\n",
    "                anomaly_embeddings.append(embedding + self.anomaly_prompt_embeddings[i])\n",
    "        material_anomaly_weights = F.softmax(self.material_anomaly_weights, dim=0)\n",
    "        for i, template in enumerate(Config.material_anomaly_templates):\n",
    "            for prop in self.get_material_properties(category_name):\n",
    "                for anomaly_type in anomaly_types[:2]:\n",
    "                    prompt = template.format(category=category_name, material_property=prop, anomaly_type=anomaly_type)\n",
    "                    embedding = self.encode_text([prompt])\n",
    "                    anomaly_embeddings.append(embedding + self.material_anomaly_embeddings[i])\n",
    "        normal_embedding = torch.cat(normal_embeddings).mean(dim=0, keepdim=True) if normal_embeddings else self.encode_text([f\"a photo of normal {category_name}\"])\n",
    "        anomaly_embedding = torch.cat(anomaly_embeddings).mean(dim=0, keepdim=True) if anomaly_embeddings else self.encode_text([f\"a photo of {category_name} with anomaly\"])\n",
    "        return normal_embedding, anomaly_embedding\n",
    "    def forward(self, image, category_name):\n",
    "        image_embedding, patch_features = self.encode_image(image)\n",
    "        normal_embedding, anomaly_embedding = self.generate_prompts(category_name)\n",
    "        normal_similarity = F.cosine_similarity(image_embedding, normal_embedding)\n",
    "        anomaly_similarity = F.cosine_similarity(image_embedding, anomaly_embedding)\n",
    "        anomaly_map = self.anomaly_localization_head(patch_features)\n",
    "        anomaly_map = F.interpolate(anomaly_map, size=Config.target_size, mode='bilinear', align_corners=False)\n",
    "        return {\n",
    "            'normal_similarity': normal_similarity,\n",
    "            'anomaly_similarity': anomaly_similarity,\n",
    "            'anomaly_score': anomaly_similarity - normal_similarity,\n",
    "            'anomaly_map': anomaly_map,\n",
    "            'image_embedding': image_embedding,\n",
    "            'normal_embedding': normal_embedding,\n",
    "            'anomaly_embedding': anomaly_embedding,\n",
    "            'patch_features': patch_features\n",
    "        }\n",
    "def train_model(model, category, train_loader, val_loader=None, num_epochs=10):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=Config.learning_rate, weight_decay=Config.weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    scaler = GradScaler() if Config.use_fp16 else None\n",
    "    best_val_loss = float('inf')\n",
    "    augmentation = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1)\n",
    "    ])\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            images = batch['image'].to(device, non_blocking=True)\n",
    "            augmented_images = augmentation(images)\n",
    "            with autocast(enabled=Config.use_fp16):\n",
    "                outputs = model(images, category)\n",
    "                aug_outputs = model(augmented_images, category)\n",
    "                contrastive_loss = F.relu(Config.margin - (outputs['anomaly_similarity'] - outputs['normal_similarity'])).mean()\n",
    "                consistency_loss = F.mse_loss(outputs['image_embedding'], aug_outputs['image_embedding'])\n",
    "                loss = contrastive_loss + 0.5 * consistency_loss\n",
    "            if Config.use_fp16:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": loss.item(), \"contrastive\": contrastive_loss.item(), \"consistency\": consistency_loss.item()})\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "        scheduler.step()\n",
    "        if val_loader:\n",
    "            val_loss = validate_model(model, category, val_loader)\n",
    "            logger.info(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                save_checkpoint(model, optimizer, epoch, val_loss, f\"{Config.checkpoint_dir}/{category}_best_model.pth\")\n",
    "        else:\n",
    "            save_checkpoint(model, optimizer, epoch, avg_train_loss, f\"{Config.checkpoint_dir}/{category}_epoch_{epoch+1}.pth\")\n",
    "    save_checkpoint(model, optimizer, num_epochs - 1, avg_train_loss, f\"{Config.checkpoint_dir}/{category}_final_model.pth\")\n",
    "    return model\n",
    "def validate_model(model, category, val_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch['image'].to(device, non_blocking=True)\n",
    "            outputs = model(images, category)\n",
    "            loss = F.relu(Config.margin - (outputs['anomaly_similarity'] - outputs['normal_similarity'])).mean()\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filename):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'loss': loss\n",
    "    }, filename)\n",
    "    logger.info(f\"Checkpoint saved to {filename}\")\n",
    "def evaluate_model(model, category, test_loader):\n",
    "    model.eval()\n",
    "    image_scores = []\n",
    "    image_labels = []\n",
    "    pixel_scores = []\n",
    "    pixel_labels = []\n",
    "    defect_specific_scores = {}\n",
    "    os.makedirs(os.path.join(Config.vis_save_dir, category), exist_ok=True)\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(test_loader, desc=f\"Evaluating {category}\")):\n",
    "            images = batch['image'].to(device, non_blocking=True)\n",
    "            labels = batch['label'].numpy()\n",
    "            masks = batch['mask'].numpy()\n",
    "            image_paths = batch['image_path']\n",
    "            defect_types = batch['defect_type']\n",
    "            outputs = model(images, category)\n",
    "            anomaly_scores = outputs['anomaly_score'].cpu().numpy()\n",
    "            anomaly_maps = outputs['anomaly_map'].squeeze(1).cpu().numpy()\n",
    "            image_scores.extend(anomaly_scores)\n",
    "            image_labels.extend(labels)\n",
    "            for idx, defect_type in enumerate(defect_types):\n",
    "                if defect_type not in defect_specific_scores:\n",
    "                    defect_specific_scores[defect_type] = {\"scores\": [], \"labels\": []}\n",
    "                defect_specific_scores[defect_type][\"scores\"].append(anomaly_scores[idx])\n",
    "                defect_specific_scores[defect_type][\"labels\"].append(labels[idx])\n",
    "            for b in range(images.shape[0]):\n",
    "                if labels[b] == 1 and masks[b].sum() > 0:\n",
    "                    pixel_scores.append(anomaly_maps[b].flatten())\n",
    "                    pixel_labels.append(masks[b].flatten())\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    image_roc_auc = roc_auc_score(image_labels, image_scores)\n",
    "    pixel_roc_auc = roc_auc_score([item for sublist in pixel_labels for item in sublist],\n",
    "                                  [item for sublist in pixel_scores for item in sublist])\n",
    "    logger.info(f\"Image-level ROC AUC: {image_roc_auc:.4f}\")\n",
    "    logger.info(f\"Pixel-level ROC AUC: {pixel_roc_auc:.4f}\")\n",
    "    defect_specific_aucs = {}\n",
    "    for defect_type, data in defect_specific_scores.items():\n",
    "        if defect_type != \"none\" and len(set(data[\"labels\"])) > 1:\n",
    "            try:\n",
    "                defect_specific_aucs[defect_type] = roc_auc_score(data[\"labels\"], data[\"scores\"])\n",
    "                logger.info(f\"  {defect_type} AUC: {defect_specific_aucs[defect_type]:.4f}\")\n",
    "            except:\n",
    "                logger.warning(f\"Could not calculate AUC for defect type {defect_type}\")\n",
    "    metrics = {\n",
    "        'category': category,\n",
    "        'image_roc_auc': float(image_roc_auc),\n",
    "        'pixel_roc_auc': float(pixel_roc_auc),\n",
    "        'defect_specific_aucs': {k: float(v) for k, v in defect_specific_aucs.items()}\n",
    "    }\n",
    "    with open(os.path.join(Config.vis_save_dir, f\"{category}_metrics.json\"), 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    return metrics\n",
    "def get_transforms():\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(Config.target_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize(Config.target_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return train_transform, test_transform\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    logger.info(\"Starting improved anomaly detection pipeline...\")\n",
    "    if not torch.cuda.is_available():\n",
    "        logger.warning(\"CUDA is not available. Using CPU, which will be much slower!\")\n",
    "    all_metrics = {}\n",
    "    os.makedirs(Config.vis_save_dir, exist_ok=True)\n",
    "    os.makedirs(Config.checkpoint_dir, exist_ok=True)\n",
    "    for category in Config.categories:\n",
    "        logger.info(f\"Processing category: {category}\")\n",
    "        train_transform, test_transform = get_transforms()\n",
    "        try:\n",
    "            train_dataset = MVTecDataset(Config.dataset_path, category, is_train=True, transform=train_transform)\n",
    "            test_dataset = MVTecDataset(Config.dataset_path, category, is_train=False, transform=test_transform)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True,\n",
    "                                      num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=Config.batch_size, shuffle=False,\n",
    "                                     num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "            model = PromptBasedAnomalyDetector(Config.clip_model_name).to(device)\n",
    "            logger.info(f\"Model device: {next(model.parameters()).device}\")\n",
    "            logger.info(f\"Training model for category: {category}\")\n",
    "            model = train_model(model, category, train_loader)\n",
    "            logger.info(f\"Evaluating model for category: {category}\")\n",
    "            metrics = evaluate_model(model, category, test_loader)\n",
    "            all_metrics[category] = metrics\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process category {category}: {str(e)}\")\n",
    "            continue\n",
    "    overall_metrics = {\n",
    "        \"overall_avg_image_auc\": sum(m['image_roc_auc'] for m in all_metrics.values()) / len(all_metrics) if all_metrics else 0,\n",
    "        \"overall_avg_pixel_auc\": sum(m['pixel_roc_auc'] for m in all_metrics.values()) / len(all_metrics) if all_metrics else 0,\n",
    "        \"categories\": list(all_metrics.keys()),\n",
    "        \"category_performance\": {cat: {\"image_auc\": metrics[\"image_roc_auc\"], \"pixel_auc\": metrics[\"pixel_roc_auc\"]} for cat, metrics in all_metrics.items()}\n",
    "    }\n",
    "    defect_type_metrics = {}\n",
    "    for category, metrics in all_metrics.items():\n",
    "        if \"defect_specific_aucs\" in metrics:\n",
    "            for defect_type, auc in metrics[\"defect_specific_aucs\"].items():\n",
    "                if defect_type not in defect_type_metrics:\n",
    "                    defect_type_metrics[defect_type] = []\n",
    "                defect_type_metrics[defect_type].append(auc)\n",
    "    overall_metrics[\"defect_type_performance\"] = {\n",
    "        defect_type: sum(aucs)/len(aucs) for defect_type, aucs in defect_type_metrics.items() if aucs\n",
    "    }\n",
    "    with open(os.path.join(Config.vis_save_dir, \"overall_metrics.json\"), 'w') as f:\n",
    "        json.dump(overall_metrics, f, indent=4)\n",
    "    elapsed_time = (time.time() - start_time) / 60.0\n",
    "    logger.info(f\"Pipeline completed in {elapsed_time:.2f} minutes.\")\n",
    "if __name__ == \"__main__\":\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        logger.info(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    cudnn.benchmark = True\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ef156b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 08:50:58,588 - __main__ - INFO - Using device: cuda\n",
      "2025-05-12 08:50:58,593 - __main__ - INFO - Starting improved anomaly detection pipeline...\n",
      "2025-05-12 08:50:58,596 - __main__ - INFO - Processing category: bottle\n",
      "2025-05-12 08:50:58,597 - __main__ - INFO - Training dataset for category: bottle\n",
      "2025-05-12 08:50:58,598 - __main__ - INFO - Searching for good samples in: ./mvtec_anomaly_detection\\bottle\\train\\good\n",
      "2025-05-12 08:50:58,600 - __main__ - INFO - Found 209 training images.\n",
      "2025-05-12 08:50:58,601 - __main__ - INFO - Testing dataset for category: bottle\n",
      "2025-05-12 08:50:58,601 - __main__ - INFO - Searching for test good samples in: ./mvtec_anomaly_detection\\bottle\\test\\good\n",
      "2025-05-12 08:50:58,603 - __main__ - INFO - Found defect types for bottle: ['broken_large', 'broken_small', 'contamination']\n",
      "2025-05-12 08:50:58,604 - __main__ - INFO - Found 20 defective samples for broken_large\n",
      "2025-05-12 08:51:01,090 - __main__ - INFO - Found 22 defective samples for broken_small\n",
      "2025-05-12 08:51:01,112 - __main__ - INFO - Found 21 defective samples for contamination\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 553\u001b[0m\n\u001b[0;32m    551\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GPUs!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    552\u001b[0m cudnn\u001b[38;5;241m.\u001b[39mbenchmark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 512\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    507\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mConfig\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    508\u001b[0m                          num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, persistent_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    509\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mConfig\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    510\u001b[0m                         num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, persistent_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 512\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPromptBasedAnomalyDetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_model_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    513\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    514\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model for category: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[23], line 213\u001b[0m, in \u001b[0;36mPromptBasedAnomalyDetector.__init__\u001b[1;34m(self, clip_model_name)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, clip_model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/clip-vit-base-patch32\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28msuper\u001b[39m(PromptBasedAnomalyDetector, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_model \u001b[38;5;241m=\u001b[39m \u001b[43mCLIPModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m CLIPTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(clip_model_name)\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_encoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_model\u001b[38;5;241m.\u001b[39mtext_model\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\transformers\\modeling_utils.py:4078\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4075\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4076\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m   4077\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[1;32m-> 4078\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4080\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4081\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4082\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4083\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4084\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4085\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4086\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4087\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4088\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4089\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4090\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4091\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4092\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m   4093\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\transformers\\utils\\hub.py:266\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcached_file\u001b[39m(\n\u001b[0;32m    209\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    210\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    212\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    213\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m     file \u001b[38;5;241m=\u001b[39m cached_files(path_or_repo_id\u001b[38;5;241m=\u001b[39mpath_or_repo_id, filenames\u001b[38;5;241m=\u001b[39m[filename], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    267\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\transformers\\utils\\hub.py:424\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[1;32m--> 424\u001b[0m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    439\u001b[0m         snapshot_download(\n\u001b[0;32m    440\u001b[0m             path_or_repo_id,\n\u001b[0;32m    441\u001b[0m             allow_patterns\u001b[38;5;241m=\u001b[39mfull_filenames,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    450\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    451\u001b[0m         )\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1008\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m    989\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m    990\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1005\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1006\u001b[0m     )\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1071\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1067\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[1;32m-> 1071\u001b[0m (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[0;32m   1087\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1531\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1531\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1534\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[0;32m   1535\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1536\u001b[0m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1448\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1445\u001b[0m hf_headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1450\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1457\u001b[0m hf_raise_for_status(r)\n\u001b[0;32m   1459\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:286\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 286\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m    287\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    288\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    289\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    291\u001b[0m     )\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\file_download.py:309\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m    308\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m response \u001b[38;5;241m=\u001b[39m http_backoff(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, retry_on_exceptions\u001b[38;5;241m=\u001b[39m(), retry_on_status_codes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m429\u001b[39m,))\n\u001b[0;32m    310\u001b[0m hf_raise_for_status(response)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:310\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[0;32m    307\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m response \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:96\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[1;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\site-packages\\urllib3\\connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\http\\client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mf:\\anaconda\\envs\\pytorch_env\\lib\\ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "class Config:\n",
    "    # Dataset parameters\n",
    "    dataset_path = \"./mvtec_anomaly_detection\"\n",
    "    categories = [\n",
    "        \"bottle\", \"cable\", \"capsule\", \"carpet\", \"grid\",\n",
    "        \"hazelnut\", \"leather\", \"metal_nut\", \"pill\", \"screw\",\n",
    "        \"tile\", \"toothbrush\", \"transistor\", \"wood\", \"zipper\"\n",
    "    ]\n",
    "    material_properties = {\n",
    "        \"bottle\": [\"transparent\", \"rigid\", \"container\", \"curved surface\"],\n",
    "        \"cable\": [\"flexible\", \"elongated\", \"connector\", \"insulated\"],\n",
    "        \"capsule\": [\"small\", \"cylindrical\", \"pharmaceutical\", \"colorful\"],\n",
    "        \"carpet\": [\"textured\", \"flat\", \"fabric\", \"patterned\"],\n",
    "        \"grid\": [\"regular pattern\", \"structured\", \"geometric\", \"repetitive\"],\n",
    "        \"hazelnut\": [\"organic\", \"natural\", \"edible\", \"oval shaped\"],\n",
    "        \"leather\": [\"textured\", \"flexible\", \"natural material\", \"durable\"],\n",
    "        \"metal_nut\": [\"metallic\", \"threaded\", \"hardware\", \"circular\"],\n",
    "        \"pill\": [\"medical\", \"small\", \"uniform\", \"pharmaceutical\"],\n",
    "        \"screw\": [\"metallic\", \"threaded\", \"hardware\", \"cylindrical\"],\n",
    "        \"tile\": [\"flat\", \"hard\", \"ceramic\", \"uniform\"],\n",
    "        \"toothbrush\": [\"plastic\", \"bristled\", \"handheld\", \"hygienic\"],\n",
    "        \"transistor\": [\"electronic\", \"semiconductor\", \"small\", \"technical\"],\n",
    "        \"wood\": [\"natural\", \"grain patterned\", \"organic\", \"fibrous\"],\n",
    "        \"zipper\": [\"metal\", \"plastic\", \"fastener\", \"interlocking\"]\n",
    "    }\n",
    "    generic_anomaly_types = [\n",
    "        \"scratched\", \"broken\", \"deformed\", \"contaminated\",\n",
    "        \"discolored\", \"cracked\", \"damaged\", \"misshapen\",\n",
    "        \"stained\", \"perforated\", \"corroded\", \"bent\"\n",
    "    ]\n",
    "    specific_anomaly_types = {\n",
    "        \"bottle\": [\"dented\", \"leaking\", \"chipped\", \"malformed neck\"],\n",
    "        \"cable\": [\"exposed wire\", \"frayed\", \"kinked\", \"connector damage\"],\n",
    "        \"capsule\": [\"crushed\", \"empty\", \"incomplete filling\", \"color variation\"],\n",
    "        \"carpet\": [\"worn\", \"torn\", \"faded\", \"stained\"],\n",
    "        \"grid\": [\"distorted\", \"misaligned\", \"broken lines\", \"irregular spacing\"],\n",
    "        \"hazelnut\": [\"moldy\", \"shriveled\", \"discolored\", \"pest damaged\"],\n",
    "        \"leather\": [\"scratched\", \"stained\", \"wrinkled\", \"uneven texture\"],\n",
    "        \"metal_nut\": [\"rusted\", \"cracked\", \"thread damage\", \"deformed\"],\n",
    "        \"pill\": [\"chipped\", \"split\", \"faded\", \"stained\"],\n",
    "        \"screw\": [\"stripped head\", \"bent\", \"rusted\", \"shortened\"],\n",
    "        \"tile\": [\"chipped\", \"cracked\", \"rough surface\", \"uneven color\"],\n",
    "        \"toothbrush\": [\"bent bristles\", \"missing bristles\", \"discolored\", \"malformed handle\"],\n",
    "        \"transistor\": [\"bent pins\", \"cracked casing\", \"missing components\", \"burn marks\"],\n",
    "        \"wood\": [\"splintered\", \"rotted\", \"knotted\", \"warped\"],\n",
    "        \"zipper\": [\"missing teeth\", \"detached\", \"stuck\", \"bent teeth\"]\n",
    "    }\n",
    "    target_size = (224, 224)\n",
    "    batch_size = 32\n",
    "    num_epochs = 10\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 1e-5\n",
    "    clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "    embedding_dim = 512\n",
    "    margin = 0.5\n",
    "    temperature = 0.07\n",
    "    anomaly_threshold = 0.5\n",
    "    vis_save_dir = \"visualization_results\"\n",
    "    checkpoint_dir = \"checkpoints\"\n",
    "\n",
    "    normal_prompt_templates = [\n",
    "        \"a photo of normal {category}\",\n",
    "        \"a normal {category} without defects\",\n",
    "        \"a pristine {category} in perfect condition\",\n",
    "        \"a flawless {category} with no anomalies\"\n",
    "    ]\n",
    "\n",
    "    anomaly_prompt_templates = [\n",
    "        \"a photo of {category} with {anomaly_type}\",\n",
    "        \"a defective {category} showing {anomaly_type}\",\n",
    "        \"a {category} that has {anomaly_type} areas\",\n",
    "        \"a damaged {category} with visible {anomaly_type}\"\n",
    "    ]\n",
    "\n",
    "    use_fp16 = True\n",
    "    num_workers = 0\n",
    "\n",
    "\n",
    "class MVTecDataset(Dataset):\n",
    "    def __init__(self, root_dir, category, is_train=True, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.category = category\n",
    "        self.is_train = is_train\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.mask_paths = []\n",
    "        self.defect_types = []\n",
    "\n",
    "        logger.info(f\"{'Training' if is_train else 'Testing'} dataset for category: {category}\")\n",
    "        if is_train:\n",
    "            train_good_path = os.path.join(root_dir, category, \"train\", \"good\")\n",
    "            logger.info(f\"Searching for good samples in: {train_good_path}\")\n",
    "            self.image_paths = sorted(glob.glob(os.path.join(train_good_path, \"*.png\")))\n",
    "            logger.info(f\"Found {len(self.image_paths)} training images.\")\n",
    "            if len(self.image_paths) == 0:\n",
    "                raise ValueError(f\"No training images found for category '{category}'\")\n",
    "            self.labels = [0] * len(self.image_paths)\n",
    "            self.mask_paths = [None] * len(self.image_paths)\n",
    "            self.defect_types = [\"none\"] * len(self.image_paths)\n",
    "        else:\n",
    "            test_good_path = os.path.join(root_dir, category, \"test\", \"good\")\n",
    "            logger.info(f\"Searching for test good samples in: {test_good_path}\")\n",
    "            good_paths = sorted(glob.glob(os.path.join(test_good_path, \"*.png\")))\n",
    "            self.image_paths.extend(good_paths)\n",
    "            self.labels.extend([0] * len(good_paths))\n",
    "            self.defect_types.extend([\"none\"] * len(good_paths))\n",
    "\n",
    "            defect_types = [\n",
    "                d for d in os.listdir(os.path.join(root_dir, category, \"test\"))\n",
    "                if d != \"good\" and os.path.isdir(os.path.join(root_dir, category, \"test\", d))\n",
    "            ]\n",
    "            logger.info(f\"Found defect types for {category}: {defect_types}\")\n",
    "\n",
    "            for defect_type in defect_types:\n",
    "                defect_path = os.path.join(root_dir, category, \"test\", defect_type)\n",
    "                defect_paths = sorted(glob.glob(os.path.join(defect_path, \"*.png\")))\n",
    "                logger.info(f\"Found {len(defect_paths)} defective samples for {defect_type}\")\n",
    "                self.image_paths.extend(defect_paths)\n",
    "                self.labels.extend([1] * len(defect_paths))\n",
    "                self.defect_types.extend([defect_type] * len(defect_paths))\n",
    "\n",
    "                for img_path in defect_paths:\n",
    "                    img_filename = os.path.basename(img_path)\n",
    "                    mask_path = os.path.join(\n",
    "                        root_dir, category, \"ground_truth\", defect_type, img_filename\n",
    "                    )\n",
    "                    mask_with_suffix = os.path.splitext(mask_path)[0] + \"_mask.png\"\n",
    "                    if os.path.exists(mask_path):\n",
    "                        self.mask_paths.append(mask_path)\n",
    "                    elif os.path.exists(mask_with_suffix):\n",
    "                        self.mask_paths.append(mask_with_suffix)\n",
    "                    else:\n",
    "                        self.mask_paths.append(None)\n",
    "                        logger.warning(f\"No mask found for {img_path}. Using placeholder.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image_path = self.image_paths[idx]\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            assert self.transform is not None, \"Transform must be provided!\"\n",
    "            image = self.transform(image)\n",
    "            label = self.labels[idx]\n",
    "            defect_type = self.defect_types[idx]\n",
    "            mask_path = self.mask_paths[idx]\n",
    "            mask = None\n",
    "            if mask_path and os.path.exists(mask_path):\n",
    "                mask = Image.open(mask_path).convert(\"L\")\n",
    "                mask = transforms.Resize(Config.target_size)(mask)\n",
    "                mask = transforms.ToTensor()(mask)\n",
    "            else:\n",
    "                mask = torch.zeros((1, *Config.target_size))  # Placeholder\n",
    "            return {\n",
    "                \"image\": image,\n",
    "                \"label\": label,\n",
    "                \"mask\": mask,\n",
    "                \"image_path\": image_path,\n",
    "                \"defect_type\": defect_type\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading sample {idx} from {self.image_paths[idx]}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "class PromptLearner(nn.Module):\n",
    "    def __init__(self, dim, prompt_len=5):\n",
    "        super().__init__()\n",
    "        self.prefix = nn.Parameter(torch.randn(1, prompt_len, dim))\n",
    "        self.suffix = nn.Parameter(torch.randn(1, prompt_len, dim))\n",
    "\n",
    "    def forward(self, text_emb):\n",
    "        prefix = self.prefix.expand(text_emb.shape[0], -1, -1)\n",
    "        suffix = self.suffix.expand(text_emb.shape[0], -1, -1)\n",
    "        return torch.cat([prefix, text_emb, suffix], dim=1)\n",
    "\n",
    "\n",
    "class PromptBasedAnomalyDetector(nn.Module):\n",
    "    def __init__(self, clip_model_name=\"openai/clip-vit-base-patch32\"):\n",
    "        super(PromptBasedAnomalyDetector, self).__init__()\n",
    "        self.clip_model = CLIPModel.from_pretrained(clip_model_name)\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(clip_model_name)\n",
    "        self.text_encoder = self.clip_model.text_model\n",
    "        self.vision_encoder = self.clip_model.vision_model\n",
    "        self.text_proj = self.clip_model.text_projection\n",
    "        self.vision_proj = self.clip_model.visual_projection\n",
    "\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.prompt_learner = PromptLearner(self.text_encoder.config.hidden_size)\n",
    "\n",
    "        self.normal_prompt_embeddings = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(1, self.text_encoder.config.hidden_size, device=device))\n",
    "            for _ in range(len(Config.normal_prompt_templates))\n",
    "        ])\n",
    "\n",
    "        self.anomaly_prompt_embeddings = nn.ParameterList([\n",
    "            nn.Parameter(torch.randn(1, self.text_encoder.config.hidden_size, device=device))\n",
    "            for _ in range(len(Config.anomaly_prompt_templates))\n",
    "        ])\n",
    "\n",
    "        self.vit_layer = self.vision_encoder\n",
    "        self.anomaly_localization_head = nn.Sequential(\n",
    "            nn.Conv2d(self.vision_encoder.config.hidden_size, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 1, kernel_size=1)\n",
    "        ).to(device)\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        text_inputs = self.tokenizer(text, padding=True, return_tensors=\"pt\").to(device)\n",
    "        text_outputs = self.text_encoder(**text_inputs)\n",
    "        text_embeddings = text_outputs.last_hidden_state[:, 0, :]\n",
    "        text_embeddings = self.text_proj(text_embeddings)\n",
    "        return text_embeddings\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        vision_outputs = self.vision_encoder(image)\n",
    "        image_embeddings = vision_outputs.last_hidden_state[:, 0, :]\n",
    "        image_embeddings = self.vision_proj(image_embeddings)\n",
    "        patch_embeddings = vision_outputs.last_hidden_state[:, 1:, :]\n",
    "        batch_size = image.shape[0]\n",
    "        patch_size = 32\n",
    "        h = w = image.shape[2] // patch_size\n",
    "        patch_features = patch_embeddings.reshape(batch_size, h, w, -1).permute(0, 3, 1, 2)\n",
    "        return image_embeddings, patch_features\n",
    "\n",
    "    def get_category_specific_anomalies(self, category):\n",
    "        return Config.specific_anomaly_types.get(category, []) + Config.generic_anomaly_types\n",
    "\n",
    "    def generate_prompts(self, category_name):\n",
    "        normal_prompts = [tpl.format(category=category_name) for tpl in Config.normal_prompt_templates]\n",
    "        normal_embeddings = [prompt_emb + self.encode_text([prompt]) for prompt, prompt_emb in zip(normal_prompts, self.normal_prompt_embeddings)]\n",
    "        normal_embedding = torch.stack(normal_embeddings).mean(dim=0)\n",
    "\n",
    "        anomaly_types = self.get_category_specific_anomalies(category_name)\n",
    "        anomaly_prompts = [tpl.format(category=category_name, anomaly_type=at) for tpl in Config.anomaly_prompt_templates for at in anomaly_types]\n",
    "        anomaly_embeddings = [prompt_emb + self.encode_text([prompt]) for prompt, prompt_emb in zip(anomaly_prompts[:4], self.anomaly_prompt_embeddings)]\n",
    "        anomaly_embedding = torch.stack(anomaly_embeddings).mean(dim=0)\n",
    "\n",
    "        return normal_embedding, anomaly_embedding\n",
    "\n",
    "def forward(self, image, category):\n",
    "    image_embedding, patch_features = self.encode_image(image)\n",
    "\n",
    "    # Vision-to-Vision prompting: Add learnable visual prompts to patch embeddings\n",
    "    patched_with_prompts = self.prompt_learner(patch_features)\n",
    "\n",
    "    # Compute similarity between image embedding and prompted embeddings\n",
    "    normal_similarity = F.cosine_similarity(image_embedding, self.normal_embedding)\n",
    "    anomaly_similarity = F.cosine_similarity(image_embedding, self.anomaly_embedding)\n",
    "\n",
    "    # Generate anomaly map using semantic concatenation in localization head\n",
    "    anomaly_map = self.anomaly_localization_head(patch_features, patched_with_prompts)\n",
    "    anomaly_map = F.interpolate(anomaly_map, size=Config.target_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "    # Normalize anomaly map to [0, 1] using sigmoid\n",
    "    anomaly_score_map = torch.sigmoid(anomaly_map).squeeze(1)  # Shape: [B, H, W]\n",
    "    \n",
    "    # Global anomaly score per image\n",
    "    anomaly_score = anomaly_score_map.mean(dim=(1, 2))  # Shape: [B]\n",
    "\n",
    "    return {\n",
    "        'normal_similarity': normal_similarity,\n",
    "        'anomaly_similarity': anomaly_similarity,\n",
    "        'anomaly_score': anomaly_score,         # Image-level anomaly score\n",
    "        'anomaly_map': anomaly_score_map,       # Pixel-level anomaly map [B, H, W]\n",
    "        'image_embedding': image_embedding\n",
    "    }\n",
    "\n",
    "def contrastive_eam_loss(normal_sim, anomaly_sim, labels, margin=0.5):\n",
    "    logits = torch.stack([normal_sim, anomaly_sim], dim=1)\n",
    "    targets = labels.long()\n",
    "    ce_loss = F.cross_entropy(logits / Config.temperature, targets)\n",
    "    alignment_loss = F.relu(Config.margin - (anomaly_sim - normal_sim)).mean()\n",
    "    return ce_loss + alignment_loss\n",
    "\n",
    "\n",
    "def visualize_results(image, anomaly_map, mask, score, path, category, defect_type):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Denormalize image\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()\n",
    "    image = (image * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406])  # Denormalize\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(f\"Image\\nDefect Type: {defect_type}\")\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(f\"Anomaly Map\\n(Score: {score:.2f})\")\n",
    "    plt.imshow(anomaly_map, cmap='jet',  vmin=0, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Ground Truth Mask\")\n",
    "    plt.imshow(mask[0].numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(Config.vis_save_dir, category)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    plt.savefig(os.path.join(save_path, os.path.basename(path)))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def compute_metrics(image_scores, image_labels, pixel_scores, pixel_labels):\n",
    "    image_roc_auc = roc_auc_score(image_labels, image_scores)\n",
    "    pixel_roc_auc = roc_auc_score([item for sublist in pixel_labels for item in sublist],\n",
    "                                   [item for sublist in pixel_scores for item in sublist])\n",
    "\n",
    "    pixel_preds = (pixel_roc_auc > Config.anomaly_threshold).astype(int)\n",
    "    pixel_f1 = f1_score([item for sublist in pixel_labels for item in sublist], pixel_preds)\n",
    "    pixel_acc = accuracy_score([item for sublist in pixel_labels for item in sublist], pixel_preds)\n",
    "\n",
    "    return {\n",
    "        'image_roc_auc': float(image_roc_auc),\n",
    "        'pixel_roc_auc': float(pixel_roc_auc),\n",
    "        'pixel_f1': float(pixel_f1),\n",
    "        'pixel_accuracy': float(pixel_acc)\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(model, category, train_loader):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=Config.learning_rate, weight_decay=Config.weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Config.num_epochs)\n",
    "    scaler = GradScaler() if Config.use_fp16 else None\n",
    "\n",
    "    augmentation = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1)\n",
    "    ])\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(Config.num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Config.num_epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].float().to(device)\n",
    "            augmented_images = augmentation(images)\n",
    "\n",
    "            with autocast(enabled=Config.use_fp16):\n",
    "                outputs = model(images, category)\n",
    "                aug_outputs = model(augmented_images, category)\n",
    "\n",
    "                loss = contrastive_eam_loss(outputs['normal_similarity'], outputs['anomaly_similarity'], labels)\n",
    "                consistency_loss = F.mse_loss(outputs['anomaly_map'], aug_outputs['anomaly_map'])\n",
    "                total_loss = loss + 0.5 * consistency_loss\n",
    "\n",
    "            if Config.use_fp16:\n",
    "                scaler.scale(total_loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_loss += total_loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": total_loss.item()})\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        logger.info(f\"Epoch {epoch+1}/{Config.num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "        scheduler.step()\n",
    "\n",
    "        save_checkpoint(model, optimizer, epoch, avg_train_loss, f\"{Config.checkpoint_dir}/{category}_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, category, test_loader):\n",
    "    model.eval()\n",
    "    image_scores = []\n",
    "    image_labels = []\n",
    "    pixel_scores = []\n",
    "    pixel_labels = []\n",
    "    defect_specific_scores = {}\n",
    "\n",
    "    os.makedirs(os.path.join(Config.vis_save_dir, category), exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(test_loader, desc=f\"Evaluating {category}\")):\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].numpy()\n",
    "            masks = batch['mask'].numpy()\n",
    "            image_paths = batch['image_path']\n",
    "            defect_types = batch['defect_type']\n",
    "\n",
    "            outputs = model(images, category)\n",
    "            anomaly_scores = outputs['anomaly_score'].cpu().numpy()\n",
    "            anomaly_maps = outputs['anomaly_map'].squeeze(1).cpu().numpy()\n",
    "\n",
    "            image_scores.extend(anomaly_scores)\n",
    "            image_labels.extend(labels)\n",
    "\n",
    "            for idx, defect_type in enumerate(defect_types):\n",
    "                if defect_type not in defect_specific_scores:\n",
    "                    defect_specific_scores[defect_type] = {\"scores\": [], \"labels\": []}\n",
    "                defect_specific_scores[defect_type][\"scores\"].append(anomaly_scores[idx])\n",
    "                defect_specific_scores[defect_type][\"labels\"].append(labels[idx])\n",
    "\n",
    "            for b in range(images.shape[0]):\n",
    "                if labels[b] == 1 and masks[b].sum() > 0:\n",
    "                    pixel_scores.append(anomaly_maps[b].flatten())\n",
    "                    pixel_labels.append(masks[b].flatten().numpy())\n",
    "\n",
    "                visualize_results(images[b], anomaly_maps[b], masks[b], anomaly_scores[b], image_paths[b], category, defect_types[b])\n",
    "\n",
    "    metrics = compute_metrics(image_scores, image_labels, pixel_scores, pixel_labels)\n",
    "    metrics['category'] = category\n",
    "\n",
    "    defect_specific_aucs = {}\n",
    "    for defect_type, data in defect_specific_scores.items():\n",
    "        if defect_type != \"none\" and len(set(data[\"labels\"])) > 1:\n",
    "            try:\n",
    "                defect_specific_aucs[defect_type] = roc_auc_score(data[\"labels\"], data[\"scores\"])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    metrics['defect_specific_aucs'] = {k: float(v) for k, v in defect_specific_aucs.items()}\n",
    "\n",
    "    with open(os.path.join(Config.vis_save_dir, f\"{category}_metrics.json\"), 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filename):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'loss': loss\n",
    "    }, filename)\n",
    "    logger.info(f\"Checkpoint saved to {filename}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    logger.info(\"Starting improved anomaly detection pipeline...\")\n",
    "    if not torch.cuda.is_available():\n",
    "        logger.warning(\"CUDA is not available. Using CPU, which will be much slower!\")\n",
    "    all_metrics = {}\n",
    "    os.makedirs(Config.vis_save_dir, exist_ok=True)\n",
    "    os.makedirs(Config.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    for category in Config.categories:\n",
    "        logger.info(f\"Processing category: {category}\")\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize(Config.target_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        test_transform = transforms.Compose([\n",
    "            transforms.Resize(Config.target_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        try:\n",
    "            train_dataset = MVTecDataset(Config.dataset_path, category, is_train=True, transform=train_transform)\n",
    "            test_dataset = MVTecDataset(Config.dataset_path, category, is_train=False, transform=test_transform)\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True,\n",
    "                                     num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=Config.batch_size, shuffle=False,\n",
    "                                    num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "\n",
    "            model = PromptBasedAnomalyDetector(Config.clip_model_name).to(device)\n",
    "            logger.info(f\"Model device: {next(model.parameters()).device}\")\n",
    "            logger.info(f\"Training model for category: {category}\")\n",
    "            model = train_model(model, category, train_loader)\n",
    "            logger.info(f\"Evaluating model for category: {category}\")\n",
    "            metrics = evaluate_model(model, category, test_loader)\n",
    "            all_metrics[category] = metrics\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process category {category}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    overall_metrics = {\n",
    "        \"overall_avg_image_auc\": sum(m['image_roc_auc'] for m in all_metrics.values()) / len(all_metrics) if all_metrics else 0,\n",
    "        \"overall_avg_pixel_auc\": sum(m['pixel_roc_auc'] for m in all_metrics.values()) / len(all_metrics) if all_metrics else 0,\n",
    "        \"categories\": list(all_metrics.keys()),\n",
    "        \"category_performance\": {cat: {\"image_auc\": metrics[\"image_roc_auc\"], \"pixel_auc\": metrics[\"pixel_roc_auc\"]} for cat, metrics in all_metrics.items()}\n",
    "    }\n",
    "\n",
    "    defect_type_metrics = {}\n",
    "    for category, metrics in all_metrics.items():\n",
    "        if \"defect_specific_aucs\" in metrics:\n",
    "            for defect_type, auc in metrics[\"defect_specific_aucs\"].items():\n",
    "                if defect_type not in defect_type_metrics:\n",
    "                    defect_type_metrics[defect_type] = []\n",
    "                defect_type_metrics[defect_type].append(auc)\n",
    "\n",
    "    overall_metrics[\"defect_type_performance\"] = {\n",
    "        defect_type: sum(aucs)/len(aucs) for defect_type, aucs in defect_type_metrics.items() if aucs\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(Config.vis_save_dir, \"overall_metrics.json\"), 'w') as f:\n",
    "        json.dump(overall_metrics, f, indent=4)\n",
    "\n",
    "    elapsed_time = (time.time() - start_time) / 60.0\n",
    "    logger.info(f\"Pipeline completed in {elapsed_time:.2f} minutes.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        logger.info(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    cudnn.benchmark = True\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad323684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 09:53:16,511 - __main__ - INFO - Using device: cuda\n",
      "2025-05-12 09:53:16,515 - __main__ - INFO - Starting improved anomaly detection pipeline...\n",
      "2025-05-12 09:53:16,516 - __main__ - INFO - Processing category: bottle\n",
      "2025-05-12 09:53:17,918 - __main__ - INFO - Training model for category: bottle\n",
      "Epoch 1/30: 100%|██████████| 14/14 [00:06<00:00,  2.31it/s, loss=0.861]\n",
      "2025-05-12 09:53:23,981 - __main__ - INFO - Epoch 1/30, Train Loss: 0.9008\n",
      "2025-05-12 09:53:25,715 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_1.pth\n",
      "Epoch 2/30: 100%|██████████| 14/14 [00:05<00:00,  2.44it/s, loss=0.837]\n",
      "2025-05-12 09:53:31,449 - __main__ - INFO - Epoch 2/30, Train Loss: 0.8470\n",
      "2025-05-12 09:53:34,318 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_2.pth\n",
      "Epoch 3/30: 100%|██████████| 14/14 [00:05<00:00,  2.40it/s, loss=0.824]\n",
      "2025-05-12 09:53:40,166 - __main__ - INFO - Epoch 3/30, Train Loss: 0.8296\n",
      "2025-05-12 09:53:43,045 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_3.pth\n",
      "Epoch 4/30: 100%|██████████| 14/14 [00:05<00:00,  2.38it/s, loss=0.816]\n",
      "2025-05-12 09:53:48,942 - __main__ - INFO - Epoch 4/30, Train Loss: 0.8193\n",
      "2025-05-12 09:53:52,330 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_4.pth\n",
      "Epoch 5/30: 100%|██████████| 14/14 [00:05<00:00,  2.38it/s, loss=0.808]\n",
      "2025-05-12 09:53:58,230 - __main__ - INFO - Epoch 5/30, Train Loss: 0.8114\n",
      "2025-05-12 09:54:03,011 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_5.pth\n",
      "Epoch 6/30: 100%|██████████| 14/14 [00:05<00:00,  2.47it/s, loss=0.802]\n",
      "2025-05-12 09:54:08,692 - __main__ - INFO - Epoch 6/30, Train Loss: 0.8050\n",
      "2025-05-12 09:54:16,962 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_6.pth\n",
      "Epoch 7/30: 100%|██████████| 14/14 [00:05<00:00,  2.39it/s, loss=0.797]\n",
      "2025-05-12 09:54:22,822 - __main__ - INFO - Epoch 7/30, Train Loss: 0.7996\n",
      "2025-05-12 09:54:27,604 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_7.pth\n",
      "Epoch 8/30: 100%|██████████| 14/14 [00:05<00:00,  2.35it/s, loss=0.793]\n",
      "2025-05-12 09:54:33,557 - __main__ - INFO - Epoch 8/30, Train Loss: 0.7949\n",
      "2025-05-12 09:54:39,610 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_8.pth\n",
      "Epoch 9/30: 100%|██████████| 14/14 [00:05<00:00,  2.38it/s, loss=0.789]\n",
      "2025-05-12 09:54:45,486 - __main__ - INFO - Epoch 9/30, Train Loss: 0.7906\n",
      "2025-05-12 09:54:51,285 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_9.pth\n",
      "Epoch 10/30: 100%|██████████| 14/14 [00:06<00:00,  2.33it/s, loss=0.785]\n",
      "2025-05-12 09:54:57,303 - __main__ - INFO - Epoch 10/30, Train Loss: 0.7868\n",
      "2025-05-12 09:55:00,606 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_10.pth\n",
      "Epoch 11/30: 100%|██████████| 14/14 [00:05<00:00,  2.40it/s, loss=0.782]\n",
      "2025-05-12 09:55:06,452 - __main__ - INFO - Epoch 11/30, Train Loss: 0.7834\n",
      "2025-05-12 09:55:09,262 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_11.pth\n",
      "Epoch 12/30: 100%|██████████| 14/14 [00:05<00:00,  2.41it/s, loss=0.779]\n",
      "2025-05-12 09:55:15,066 - __main__ - INFO - Epoch 12/30, Train Loss: 0.7802\n",
      "2025-05-12 09:55:19,547 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_12.pth\n",
      "Epoch 13/30: 100%|██████████| 14/14 [00:05<00:00,  2.40it/s, loss=0.776]\n",
      "2025-05-12 09:55:25,389 - __main__ - INFO - Epoch 13/30, Train Loss: 0.7775\n",
      "2025-05-12 09:55:28,305 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_13.pth\n",
      "Epoch 14/30: 100%|██████████| 14/14 [00:05<00:00,  2.39it/s, loss=0.774]\n",
      "2025-05-12 09:55:34,162 - __main__ - INFO - Epoch 14/30, Train Loss: 0.7750\n",
      "2025-05-12 09:55:38,913 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_14.pth\n",
      "Epoch 15/30: 100%|██████████| 14/14 [00:05<00:00,  2.39it/s, loss=0.772]\n",
      "2025-05-12 09:55:44,768 - __main__ - INFO - Epoch 15/30, Train Loss: 0.7728\n",
      "2025-05-12 09:55:47,871 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_15.pth\n",
      "Epoch 16/30: 100%|██████████| 14/14 [00:05<00:00,  2.41it/s, loss=0.77] \n",
      "2025-05-12 09:55:53,678 - __main__ - INFO - Epoch 16/30, Train Loss: 0.7709\n",
      "2025-05-12 09:55:56,874 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_16.pth\n",
      "Epoch 17/30: 100%|██████████| 14/14 [00:06<00:00,  2.33it/s, loss=0.768]\n",
      "2025-05-12 09:56:02,891 - __main__ - INFO - Epoch 17/30, Train Loss: 0.7692\n",
      "2025-05-12 09:56:05,611 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_17.pth\n",
      "Epoch 18/30: 100%|██████████| 14/14 [00:05<00:00,  2.40it/s, loss=0.767]\n",
      "2025-05-12 09:56:11,437 - __main__ - INFO - Epoch 18/30, Train Loss: 0.7677\n",
      "2025-05-12 09:56:14,112 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_18.pth\n",
      "Epoch 19/30: 100%|██████████| 14/14 [00:05<00:00,  2.39it/s, loss=0.766]\n",
      "2025-05-12 09:56:19,977 - __main__ - INFO - Epoch 19/30, Train Loss: 0.7665\n",
      "2025-05-12 09:56:23,363 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_19.pth\n",
      "Epoch 20/30: 100%|██████████| 14/14 [00:05<00:00,  2.38it/s, loss=0.765]\n",
      "2025-05-12 09:56:29,242 - __main__ - INFO - Epoch 20/30, Train Loss: 0.7654\n",
      "2025-05-12 09:56:32,363 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_20.pth\n",
      "Epoch 21/30: 100%|██████████| 14/14 [00:05<00:00,  2.35it/s, loss=0.764]\n",
      "2025-05-12 09:56:38,321 - __main__ - INFO - Epoch 21/30, Train Loss: 0.7645\n",
      "2025-05-12 09:56:41,221 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_21.pth\n",
      "Epoch 22/30: 100%|██████████| 14/14 [00:05<00:00,  2.39it/s, loss=0.763]\n",
      "2025-05-12 09:56:47,087 - __main__ - INFO - Epoch 22/30, Train Loss: 0.7638\n",
      "2025-05-12 09:56:50,099 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_22.pth\n",
      "Epoch 23/30: 100%|██████████| 14/14 [00:05<00:00,  2.39it/s, loss=0.763]\n",
      "2025-05-12 09:56:55,956 - __main__ - INFO - Epoch 23/30, Train Loss: 0.7632\n",
      "2025-05-12 09:56:58,902 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_23.pth\n",
      "Epoch 24/30: 100%|██████████| 14/14 [00:05<00:00,  2.38it/s, loss=0.763]\n",
      "2025-05-12 09:57:04,779 - __main__ - INFO - Epoch 24/30, Train Loss: 0.7627\n",
      "2025-05-12 09:57:07,927 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_24.pth\n",
      "Epoch 25/30: 100%|██████████| 14/14 [00:05<00:00,  2.40it/s, loss=0.762]\n",
      "2025-05-12 09:57:13,754 - __main__ - INFO - Epoch 25/30, Train Loss: 0.7623\n",
      "2025-05-12 09:57:16,677 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_25.pth\n",
      "Epoch 26/30: 100%|██████████| 14/14 [00:05<00:00,  2.40it/s, loss=0.762]\n",
      "2025-05-12 09:57:22,504 - __main__ - INFO - Epoch 26/30, Train Loss: 0.7620\n",
      "2025-05-12 09:57:25,541 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_26.pth\n",
      "Epoch 27/30: 100%|██████████| 14/14 [00:05<00:00,  2.35it/s, loss=0.762]\n",
      "2025-05-12 09:57:31,510 - __main__ - INFO - Epoch 27/30, Train Loss: 0.7618\n",
      "2025-05-12 09:57:34,509 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_27.pth\n",
      "Epoch 28/30: 100%|██████████| 14/14 [00:05<00:00,  2.39it/s, loss=0.762]\n",
      "2025-05-12 09:57:40,373 - __main__ - INFO - Epoch 28/30, Train Loss: 0.7617\n",
      "2025-05-12 09:57:41,933 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_28.pth\n",
      "Epoch 29/30: 100%|██████████| 14/14 [00:05<00:00,  2.46it/s, loss=0.762]\n",
      "2025-05-12 09:57:47,623 - __main__ - INFO - Epoch 29/30, Train Loss: 0.7617\n",
      "2025-05-12 09:57:52,439 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_29.pth\n",
      "Epoch 30/30: 100%|██████████| 14/14 [00:05<00:00,  2.51it/s, loss=0.762]\n",
      "2025-05-12 09:57:58,031 - __main__ - INFO - Epoch 30/30, Train Loss: 0.7616\n",
      "2025-05-12 09:58:02,192 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_30.pth\n",
      "2025-05-12 09:58:02,193 - __main__ - INFO - Evaluating model for category: bottle\n",
      "Evaluating bottle: 100%|██████████| 6/6 [00:32<00:00,  5.38s/it]\n",
      "2025-05-12 09:58:35,560 - __main__ - INFO - Pipeline completed in 5.32 minutes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "class Config:\n",
    "    dataset_path = \"./data\"\n",
    "    categories = [\"bottle\"]\n",
    "    target_size = (224, 224)\n",
    "    batch_size = 16\n",
    "    num_epochs = 30\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 1e-5\n",
    "    clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "    embedding_dim = 512\n",
    "    margin = 0.5\n",
    "    temperature = 0.07\n",
    "    anomaly_threshold = 0.5\n",
    "    pixel_weight = 0.5\n",
    "    vis_save_dir = \"visualization_results\"\n",
    "    checkpoint_dir = \"checkpoints\"\n",
    "    use_fp16 = True\n",
    "    num_workers = 0\n",
    "\n",
    "    # Anomaly types\n",
    "    generic_anomaly_types = [\"scratched\", \"broken\", \"contaminated\", \"discolored\"]\n",
    "    specific_anomaly_types = {\n",
    "        \"bottle\": [\"dented\", \"leaking\", \"chipped\"],\n",
    "        \"cable\": [\"frayed\", \"kinked\"],\n",
    "        \"capsule\": [\"crushed\", \"color variation\"],\n",
    "        \"carpet\": [\"worn\", \"torn\"],\n",
    "        \"grid\": [\"distorted\", \"misaligned\"]\n",
    "    }\n",
    "\n",
    "class MVTecDataset(Dataset):\n",
    "    def __init__(self, root_dir, category, is_train=True, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.category = category\n",
    "        self.is_train = is_train\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.mask_paths = []\n",
    "        self.defect_types = []\n",
    "\n",
    "        if is_train:\n",
    "            train_good_path = os.path.join(root_dir, category, \"train\", \"good\")\n",
    "            self.image_paths = sorted(glob.glob(os.path.join(train_good_path, \"*.png\")))\n",
    "            self.labels = [0] * len(self.image_paths)\n",
    "            self.mask_paths = [None] * len(self.image_paths)\n",
    "            self.defect_types = [\"none\"] * len(self.image_paths)\n",
    "        else:\n",
    "            test_good_path = os.path.join(root_dir, category, \"test\", \"good\")\n",
    "            good_paths = sorted(glob.glob(os.path.join(test_good_path, \"*.png\")))\n",
    "            self.image_paths.extend(good_paths)\n",
    "            self.labels.extend([0] * len(good_paths))\n",
    "            self.defect_types.extend([\"none\"] * len(good_paths))\n",
    "            self.mask_paths.extend([None] * len(good_paths))\n",
    "            defect_types = [\n",
    "                d for d in os.listdir(os.path.join(root_dir, category, \"test\"))\n",
    "                if d != \"good\" and os.path.isdir(os.path.join(root_dir, category, \"test\", d))\n",
    "            ]\n",
    "            for defect_type in defect_types:\n",
    "                defect_path = os.path.join(root_dir, category, \"test\", defect_type)\n",
    "                defect_paths = sorted(glob.glob(os.path.join(defect_path, \"*.png\")))\n",
    "                self.image_paths.extend(defect_paths)\n",
    "                self.labels.extend([1] * len(defect_paths))\n",
    "                self.defect_types.extend([defect_type] * len(defect_paths))\n",
    "                for img_path in defect_paths:\n",
    "                    img_filename = os.path.basename(img_path)\n",
    "                    mask_path = os.path.join(\n",
    "                        root_dir, category, \"ground_truth\", defect_type, img_filename\n",
    "                    )\n",
    "                    mask_with_suffix = os.path.splitext(mask_path)[0] + \"_mask.png\"\n",
    "                    if os.path.exists(mask_path):\n",
    "                        self.mask_paths.append(mask_path)\n",
    "                    elif os.path.exists(mask_with_suffix):\n",
    "                        self.mask_paths.append(mask_with_suffix)\n",
    "                    else:\n",
    "                        self.mask_paths.append(None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        defect_type = self.defect_types[idx]\n",
    "        mask = None\n",
    "\n",
    "        if self.mask_paths[idx] and os.path.exists(self.mask_paths[idx]):\n",
    "            mask = Image.open(self.mask_paths[idx]).convert(\"L\")\n",
    "            mask = transforms.Resize(Config.target_size)(mask)\n",
    "            mask = transforms.ToTensor()(mask)\n",
    "        else:\n",
    "            mask = torch.zeros((1, *Config.target_size))  # Default empty mask\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"label\": label,\n",
    "            \"mask\": mask,\n",
    "            \"image_path\": image_path,\n",
    "            \"defect_type\": defect_type\n",
    "        }\n",
    "\n",
    "class VisionPromptLearner(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.embed_dim = clip_model.vision_model.config.hidden_size\n",
    "        self.prompt_length = 5\n",
    "        self.visual_prompts = nn.Parameter(torch.randn(self.prompt_length, self.embed_dim))\n",
    "\n",
    "    def forward(self, patch_embeddings):\n",
    "        prompts = self.visual_prompts.expand(patch_embeddings.shape[0], -1, -1)\n",
    "        return torch.cat([prompts, patch_embeddings], dim=1)\n",
    "\n",
    "class AnomalyLocalizationHead(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim * 2, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 1, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, patch_features, prompts):\n",
    "        batch_size, seq_len, embed_dim = prompts.shape\n",
    "        h = w = int(patch_features.shape[1]**0.5)\n",
    "        \n",
    "        # Take mean of prompts across sequence dimension\n",
    "        prompts_global = prompts.mean(dim=1)  # (batch_size, embed_dim)\n",
    "        # Expand to (batch_size, embed_dim, 1, 1) and tile to (h, w)\n",
    "        prompts_expanded = prompts_global.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, h, w)\n",
    "        \n",
    "        # Reshape patch_features to (batch_size, embed_dim, h, w)\n",
    "        patch_reshaped = patch_features.view(batch_size, embed_dim, h, w)\n",
    "        \n",
    "        # Concatenate along channel dimension\n",
    "        fused = torch.cat([patch_reshaped, prompts_expanded], dim=1)\n",
    "        return self.fusion_layer(fused)\n",
    "\n",
    "class PromptBasedAnomalyDetector(nn.Module):\n",
    "    def __init__(self, clip_model_name=\"openai/clip-vit-base-patch32\"):\n",
    "        super().__init__()\n",
    "        self.clip_model = CLIPModel.from_pretrained(clip_model_name)\n",
    "        self.vision_encoder = self.clip_model.vision_model\n",
    "        self.vision_proj = self.clip_model.visual_projection\n",
    "        self.prompt_learner = VisionPromptLearner(self.clip_model)\n",
    "        self.anomaly_localization_head = AnomalyLocalizationHead(self.vision_encoder.config.hidden_size)\n",
    "\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        vision_outputs = self.vision_encoder(image)\n",
    "        image_embeddings = vision_outputs.last_hidden_state[:, 0, :]\n",
    "        image_embeddings = self.vision_proj(image_embeddings)\n",
    "        patch_embeddings = vision_outputs.last_hidden_state[:, 1:, :]\n",
    "        return image_embeddings, patch_embeddings\n",
    "\n",
    "    def forward(self, image, category):\n",
    "        image_embedding, patch_embeddings = self.encode_image(image)\n",
    "        patched_with_prompts = self.prompt_learner(patch_embeddings)\n",
    "        anomaly_map = self.anomaly_localization_head(patch_embeddings, patched_with_prompts)\n",
    "        anomaly_map = F.interpolate(anomaly_map, size=Config.target_size, mode='bilinear', align_corners=False)\n",
    "        anomaly_map = anomaly_map.squeeze(1)\n",
    "\n",
    "        # ✅ Apply sigmoid to normalize between [0, 1]\n",
    "        normalized_anomaly_map = torch.sigmoid(anomaly_map)\n",
    "        anomaly_score = normalized_anomaly_map.mean(dim=(1, 2))\n",
    "\n",
    "        return {\n",
    "            'anomaly_score': anomaly_score,\n",
    "            'anomaly_map': normalized_anomaly_map,  # ✅ Already in [0, 1]\n",
    "            'image_embedding': image_embedding\n",
    "        }\n",
    "def contrastive_eam_loss(anomaly_scores, labels, anomaly_maps, masks):\n",
    "    # Image-level loss\n",
    "    logits = anomaly_scores.unsqueeze(1)\n",
    "    targets = labels.float().unsqueeze(1)\n",
    "    image_loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "\n",
    "    # Pixel-level loss\n",
    "    if masks is not None and masks.shape == anomaly_maps.shape:\n",
    "        # If masks are (B, H, W), expand to (B, 1, H, W) if necessary\n",
    "        if masks.dim() == 3:\n",
    "            masks = masks.unsqueeze(1)  # For BCEWithLogitsLoss which expects channel dim\n",
    "        elif masks.dim() == 4 and masks.shape[1] != 1:\n",
    "            masks = masks[:, 0].unsqueeze(1)  # Take first channel if multi-channel\n",
    "\n",
    "        pixel_loss = F.binary_cross_entropy_with_logits(anomaly_maps, masks.float())\n",
    "        total_loss = image_loss + Config.pixel_weight * pixel_loss\n",
    "    else:\n",
    "        total_loss = image_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def visualize_results(image, anomaly_map, mask, score, path, category, defect_type):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    # Denormalize image\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()\n",
    "    image = (image * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406])\n",
    "    image = np.clip(image, 0, 1)\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    defect_title = defect_type.capitalize() if defect_type != \"none\" else \"Normal\"\n",
    "    plt.title(f\"Image\\nDefect Type: {defect_title}\")\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(f\"Anomaly Map\\n(Score: {score:.2f})\")\n",
    "    plt.imshow(anomaly_map, cmap='jet',  vmin=0, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Ground Truth Mask\")\n",
    "    if mask is not None and mask.sum() > 0:\n",
    "        plt.imshow(mask[0], cmap='gray')  # ✅ No .numpy() needed\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, \"No Mask\", ha='center', va='center')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(Config.vis_save_dir, category)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    plt.savefig(os.path.join(save_path, os.path.basename(path)))\n",
    "    plt.close()\n",
    "\n",
    "def compute_metrics(image_scores, image_labels, pixel_scores, pixel_labels):\n",
    "    # Image-level ROC-AUC\n",
    "    image_roc_auc = 0.0\n",
    "    if len(set(image_labels)) > 1:\n",
    "        image_roc_auc = roc_auc_score(image_labels, image_scores)\n",
    "\n",
    "    # Pixel-level metrics\n",
    "    pixel_roc_auc = 0.0\n",
    "    pixel_f1 = 0.0\n",
    "    pixel_acc = 0.0\n",
    "\n",
    "    if len(pixel_scores) > 0 and len(pixel_labels) > 0:\n",
    "        all_pixel_scores = np.concatenate(pixel_scores)\n",
    "        all_pixel_labels = np.concatenate(pixel_labels)\n",
    "\n",
    "        # Skip if only one class exists\n",
    "        if len(np.unique(all_pixel_labels)) < 2:\n",
    "            logger.warning(\"Only one class found in pixel labels. Skipping pixel-level AUC.\")\n",
    "        else:\n",
    "            # Normalize scores to [0, 1] using sigmoid\n",
    "            all_pixel_scores = 1 / (1 + np.exp(-all_pixel_scores))  # Sigmoid normalization\n",
    "            try:\n",
    "                pixel_roc_auc = roc_auc_score(all_pixel_labels, all_pixel_scores)\n",
    "            except ValueError as e:\n",
    "                logger.error(f\"Failed to compute pixel AUC: {str(e)}\")\n",
    "\n",
    "        pixel_preds = (all_pixel_scores > Config.anomaly_threshold).astype(int)\n",
    "        pixel_f1 = f1_score(all_pixel_labels, pixel_preds, zero_division=0)\n",
    "        pixel_acc = accuracy_score(all_pixel_labels, pixel_preds)\n",
    "\n",
    "    return {\n",
    "        'image_roc_auc': float(image_roc_auc),\n",
    "        'pixel_roc_auc': float(pixel_roc_auc),\n",
    "        'pixel_f1': float(pixel_f1),\n",
    "        'pixel_accuracy': float(pixel_acc)\n",
    "    }\n",
    "\n",
    "def train_model(model, category, train_loader):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=Config.learning_rate, weight_decay=Config.weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Config.num_epochs)\n",
    "    scaler = GradScaler() if Config.use_fp16 else None\n",
    "    augmentation = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1)\n",
    "    ])\n",
    "\n",
    "    for epoch in range(Config.num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Config.num_epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].float().to(device)\n",
    "            masks = batch['mask'].to(device)\n",
    "            augmented_images = augmentation(images)\n",
    "\n",
    "            with autocast(enabled=Config.use_fp16):\n",
    "                outputs = model(images, category)\n",
    "                aug_outputs = model(augmented_images, category)\n",
    "                loss = contrastive_eam_loss(\n",
    "                outputs['anomaly_score'], \n",
    "                labels, \n",
    "                outputs['anomaly_map'], \n",
    "                batch['mask'].to(device) \n",
    "            )\n",
    "                consistency_loss = F.mse_loss(outputs['anomaly_map'], aug_outputs['anomaly_map'])\n",
    "                total_loss = loss + 0.5 * consistency_loss\n",
    "\n",
    "            if Config.use_fp16:\n",
    "                scaler.scale(total_loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss += total_loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": total_loss.item()})\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        logger.info(f\"Epoch {epoch+1}/{Config.num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "        scheduler.step()\n",
    "        save_checkpoint(model, optimizer, epoch, avg_train_loss, f\"{Config.checkpoint_dir}/{category}_epoch_{epoch+1}.pth\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, category, test_loader):\n",
    "    model.eval()\n",
    "    image_scores = []\n",
    "    image_labels = []\n",
    "    pixel_scores = []\n",
    "    pixel_labels = []\n",
    "    defect_specific_scores = {}\n",
    "\n",
    "    os.makedirs(os.path.join(Config.vis_save_dir, category), exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=f\"Evaluating {category}\"):\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].cpu().numpy()\n",
    "            masks = batch['mask'].cpu().numpy()\n",
    "            image_paths = batch['image_path']\n",
    "            defect_types = batch['defect_type']\n",
    "\n",
    "            outputs = model(images, category)\n",
    "            anomaly_scores = outputs['anomaly_score'].cpu().numpy()\n",
    "            anomaly_maps = outputs['anomaly_map'].cpu().numpy()\n",
    "\n",
    "            image_scores.extend(anomaly_scores)\n",
    "            image_labels.extend(labels)\n",
    "\n",
    "            for idx, defect_type in enumerate(defect_types):\n",
    "                if defect_type not in defect_specific_scores:\n",
    "                    defect_specific_scores[defect_type] = {\"scores\": [], \"labels\": []}\n",
    "                defect_specific_scores[defect_type][\"scores\"].append(anomaly_scores[idx])\n",
    "                defect_specific_scores[defect_type][\"labels\"].append(labels[idx])\n",
    "\n",
    "            for b in range(images.shape[0]):\n",
    "                if labels[b] == 1 and masks[b].sum() > 0:\n",
    "                    pixel_scores.append(anomaly_maps[b].flatten())\n",
    "                    pixel_labels.append(masks[b].flatten().astype(int))\n",
    "                visualize_results(images[b], anomaly_maps[b], masks[b], anomaly_scores[b], image_paths[b], category, defect_types[b])\n",
    "\n",
    "    metrics = compute_metrics(image_scores, image_labels, pixel_scores, pixel_labels)\n",
    "    metrics['category'] = category\n",
    "\n",
    "    defect_specific_aucs = {}\n",
    "    for defect_type, data in defect_specific_scores.items():\n",
    "        if defect_type != \"none\" and len(set(data[\"labels\"])) > 1:\n",
    "            try:\n",
    "                defect_specific_aucs[defect_type] = roc_auc_score(data[\"labels\"], data[\"scores\"])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    metrics['defect_specific_aucs'] = {k: float(v) for k, v in defect_specific_aucs.items()}\n",
    "\n",
    "    with open(os.path.join(Config.vis_save_dir, f\"{category}_metrics.json\"), 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filename):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'loss': loss\n",
    "    }, filename)\n",
    "    logger.info(f\"Checkpoint saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    logger.info(\"Starting improved anomaly detection pipeline...\")\n",
    "    if not torch.cuda.is_available():\n",
    "        logger.warning(\"CUDA is not available. Using CPU, which will be much slower!\")\n",
    "\n",
    "    all_metrics = {}\n",
    "    os.makedirs(Config.vis_save_dir, exist_ok=True)\n",
    "    os.makedirs(Config.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    for category in Config.categories:\n",
    "        logger.info(f\"Processing category: {category}\")\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize(Config.target_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        test_transform = transforms.Compose([\n",
    "            transforms.Resize(Config.target_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        train_dataset = MVTecDataset(Config.dataset_path, category, is_train=True, transform=train_transform)\n",
    "        test_dataset = MVTecDataset(Config.dataset_path, category, is_train=False, transform=test_transform)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=Config.batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "        model = PromptBasedAnomalyDetector(Config.clip_model_name).to(device)\n",
    "        logger.info(f\"Training model for category: {category}\")\n",
    "        model = train_model(model, category, train_loader)\n",
    "        logger.info(f\"Evaluating model for category: {category}\")\n",
    "        metrics = evaluate_model(model, category, test_loader)\n",
    "        all_metrics[category] = metrics\n",
    "\n",
    "    overall_metrics = {\n",
    "        \"overall_avg_image_auc\": sum(m['image_roc_auc'] for m in all_metrics.values()) / len(all_metrics),\n",
    "        \"overall_avg_pixel_auc\": sum(m['pixel_roc_auc'] for m in all_metrics.values()) / len(all_metrics),\n",
    "        \"categories\": list(all_metrics.keys()),\n",
    "        \"category_performance\": {cat: {\"image_auc\": m[\"image_roc_auc\"], \"pixel_auc\": m[\"pixel_roc_auc\"]} for cat, m in all_metrics.items()}\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(Config.vis_save_dir, \"overall_metrics.json\"), 'w') as f:\n",
    "        json.dump(overall_metrics, f, indent=4)\n",
    "\n",
    "    elapsed_time = (time.time() - start_time) / 60.0\n",
    "    logger.info(f\"Pipeline completed in {elapsed_time:.2f} minutes.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f89468e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 11:25:12,625 - __main__ - INFO - Using device: cuda\n",
      "2025-05-12 11:25:12,630 - __main__ - INFO - Starting improved anomaly detection pipeline...\n",
      "2025-05-12 11:25:12,831 - __main__ - INFO - Processing category: bottle\n",
      "2025-05-12 11:25:26,053 - __main__ - INFO - Training model for category: bottle\n",
      "Epoch 1/50: 100%|██████████| 14/14 [00:09<00:00,  1.42it/s, loss=0.829]\n",
      "2025-05-12 11:25:35,911 - __main__ - INFO - Epoch 1/50, Train Loss: 0.8636\n",
      "2025-05-12 11:25:37,462 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_1.pth\n",
      "Epoch 2/50: 100%|██████████| 14/14 [00:05<00:00,  2.51it/s, loss=0.809]\n",
      "2025-05-12 11:25:43,038 - __main__ - INFO - Epoch 2/50, Train Loss: 0.8174\n",
      "2025-05-12 11:25:46,046 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_2.pth\n",
      "Epoch 3/50: 100%|██████████| 14/14 [00:05<00:00,  2.47it/s, loss=0.798]\n",
      "2025-05-12 11:25:51,725 - __main__ - INFO - Epoch 3/50, Train Loss: 0.8032\n",
      "2025-05-12 11:25:53,362 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_3.pth\n",
      "Epoch 4/50: 100%|██████████| 14/14 [00:05<00:00,  2.50it/s, loss=0.791]\n",
      "2025-05-12 11:25:58,971 - __main__ - INFO - Epoch 4/50, Train Loss: 0.7942\n",
      "2025-05-12 11:26:04,961 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_4.pth\n",
      "Epoch 5/50: 100%|██████████| 14/14 [00:05<00:00,  2.48it/s, loss=0.784]\n",
      "2025-05-12 11:26:10,606 - __main__ - INFO - Epoch 5/50, Train Loss: 0.7875\n",
      "2025-05-12 11:26:14,671 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_5.pth\n",
      "Epoch 6/50: 100%|██████████| 14/14 [00:05<00:00,  2.49it/s, loss=0.78] \n",
      "2025-05-12 11:26:20,289 - __main__ - INFO - Epoch 6/50, Train Loss: 0.7822\n",
      "2025-05-12 11:26:22,060 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_6.pth\n",
      "Epoch 7/50: 100%|██████████| 14/14 [00:05<00:00,  2.49it/s, loss=0.775]\n",
      "2025-05-12 11:26:27,690 - __main__ - INFO - Epoch 7/50, Train Loss: 0.7775\n",
      "2025-05-12 11:26:32,192 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_7.pth\n",
      "Epoch 8/50: 100%|██████████| 14/14 [00:05<00:00,  2.50it/s, loss=0.771]\n",
      "2025-05-12 11:26:37,786 - __main__ - INFO - Epoch 8/50, Train Loss: 0.7732\n",
      "2025-05-12 11:26:41,231 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_8.pth\n",
      "Epoch 9/50: 100%|██████████| 14/14 [00:05<00:00,  2.51it/s, loss=0.768]\n",
      "2025-05-12 11:26:46,822 - __main__ - INFO - Epoch 9/50, Train Loss: 0.7694\n",
      "2025-05-12 11:26:51,409 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_9.pth\n",
      "Epoch 10/50: 100%|██████████| 14/14 [00:05<00:00,  2.44it/s, loss=0.764]\n",
      "2025-05-12 11:26:57,154 - __main__ - INFO - Epoch 10/50, Train Loss: 0.7659\n",
      "2025-05-12 11:27:00,915 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_10.pth\n",
      "Epoch 11/50: 100%|██████████| 14/14 [00:05<00:00,  2.36it/s, loss=0.761]\n",
      "2025-05-12 11:27:06,851 - __main__ - INFO - Epoch 11/50, Train Loss: 0.7626\n",
      "2025-05-12 11:27:10,419 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_11.pth\n",
      "Epoch 12/50: 100%|██████████| 14/14 [00:05<00:00,  2.58it/s, loss=0.758]\n",
      "2025-05-12 11:27:15,857 - __main__ - INFO - Epoch 12/50, Train Loss: 0.7596\n",
      "2025-05-12 11:27:17,448 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_12.pth\n",
      "Epoch 13/50: 100%|██████████| 14/14 [00:05<00:00,  2.52it/s, loss=0.755]\n",
      "2025-05-12 11:27:23,014 - __main__ - INFO - Epoch 13/50, Train Loss: 0.7568\n",
      "2025-05-12 11:27:27,957 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_13.pth\n",
      "Epoch 14/50: 100%|██████████| 14/14 [00:05<00:00,  2.37it/s, loss=0.753]\n",
      "2025-05-12 11:27:33,882 - __main__ - INFO - Epoch 14/50, Train Loss: 0.7543\n",
      "2025-05-12 11:27:37,655 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_14.pth\n",
      "Epoch 15/50: 100%|██████████| 14/14 [00:05<00:00,  2.37it/s, loss=0.751]\n",
      "2025-05-12 11:27:43,566 - __main__ - INFO - Epoch 15/50, Train Loss: 0.7519\n",
      "2025-05-12 11:27:47,215 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_15.pth\n",
      "Epoch 16/50: 100%|██████████| 14/14 [00:05<00:00,  2.34it/s, loss=0.749]\n",
      "2025-05-12 11:27:53,211 - __main__ - INFO - Epoch 16/50, Train Loss: 0.7497\n",
      "2025-05-12 11:27:56,299 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_16.pth\n",
      "Epoch 17/50: 100%|██████████| 14/14 [00:05<00:00,  2.37it/s, loss=0.747]\n",
      "2025-05-12 11:28:02,222 - __main__ - INFO - Epoch 17/50, Train Loss: 0.7477\n",
      "2025-05-12 11:28:06,388 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_17.pth\n",
      "Epoch 18/50: 100%|██████████| 14/14 [00:05<00:00,  2.36it/s, loss=0.745]\n",
      "2025-05-12 11:28:12,327 - __main__ - INFO - Epoch 18/50, Train Loss: 0.7458\n",
      "2025-05-12 11:28:15,375 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_18.pth\n",
      "Epoch 19/50: 100%|██████████| 14/14 [00:05<00:00,  2.37it/s, loss=0.743]\n",
      "2025-05-12 11:28:21,276 - __main__ - INFO - Epoch 19/50, Train Loss: 0.7440\n",
      "2025-05-12 11:28:23,029 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_19.pth\n",
      "Epoch 20/50: 100%|██████████| 14/14 [00:05<00:00,  2.37it/s, loss=0.742]\n",
      "2025-05-12 11:28:28,945 - __main__ - INFO - Epoch 20/50, Train Loss: 0.7424\n",
      "2025-05-12 11:28:33,645 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_20.pth\n",
      "Epoch 21/50: 100%|██████████| 14/14 [00:05<00:00,  2.36it/s, loss=0.74] \n",
      "2025-05-12 11:28:39,576 - __main__ - INFO - Epoch 21/50, Train Loss: 0.7409\n",
      "2025-05-12 11:28:42,567 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_21.pth\n",
      "Epoch 22/50: 100%|██████████| 14/14 [00:05<00:00,  2.38it/s, loss=0.739]\n",
      "2025-05-12 11:28:48,457 - __main__ - INFO - Epoch 22/50, Train Loss: 0.7396\n",
      "2025-05-12 11:28:51,549 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_22.pth\n",
      "Epoch 23/50: 100%|██████████| 14/14 [00:05<00:00,  2.38it/s, loss=0.738]\n",
      "2025-05-12 11:28:57,441 - __main__ - INFO - Epoch 23/50, Train Loss: 0.7383\n",
      "2025-05-12 11:28:59,051 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_23.pth\n",
      "Epoch 24/50: 100%|██████████| 14/14 [00:05<00:00,  2.37it/s, loss=0.737]\n",
      "2025-05-12 11:29:04,973 - __main__ - INFO - Epoch 24/50, Train Loss: 0.7371\n",
      "2025-05-12 11:29:09,546 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_24.pth\n",
      "Epoch 25/50: 100%|██████████| 14/14 [00:05<00:00,  2.35it/s, loss=0.736]\n",
      "2025-05-12 11:29:15,514 - __main__ - INFO - Epoch 25/50, Train Loss: 0.7361\n",
      "2025-05-12 11:29:18,544 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_25.pth\n",
      "Epoch 26/50: 100%|██████████| 14/14 [00:05<00:00,  2.35it/s, loss=0.735]\n",
      "2025-05-12 11:29:24,500 - __main__ - INFO - Epoch 26/50, Train Loss: 0.7351\n",
      "2025-05-12 11:29:26,175 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_26.pth\n",
      "Epoch 27/50: 100%|██████████| 14/14 [00:05<00:00,  2.38it/s, loss=0.734]\n",
      "2025-05-12 11:29:32,071 - __main__ - INFO - Epoch 27/50, Train Loss: 0.7343\n",
      "2025-05-12 11:29:35,450 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_27.pth\n",
      "Epoch 28/50: 100%|██████████| 14/14 [00:05<00:00,  2.39it/s, loss=0.733]\n",
      "2025-05-12 11:29:41,318 - __main__ - INFO - Epoch 28/50, Train Loss: 0.7334\n",
      "2025-05-12 11:29:43,570 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_28.pth\n",
      "Epoch 29/50: 100%|██████████| 14/14 [00:05<00:00,  2.35it/s, loss=0.732]\n",
      "2025-05-12 11:29:49,522 - __main__ - INFO - Epoch 29/50, Train Loss: 0.7327\n",
      "2025-05-12 11:29:52,515 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_29.pth\n",
      "Epoch 30/50: 100%|██████████| 14/14 [00:05<00:00,  2.38it/s, loss=0.732]\n",
      "2025-05-12 11:29:58,411 - __main__ - INFO - Epoch 30/50, Train Loss: 0.7321\n",
      "2025-05-12 11:30:01,320 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_30.pth\n",
      "Epoch 31/50: 100%|██████████| 14/14 [00:05<00:00,  2.40it/s, loss=0.731]\n",
      "2025-05-12 11:30:07,167 - __main__ - INFO - Epoch 31/50, Train Loss: 0.7315\n",
      "2025-05-12 11:30:09,510 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_31.pth\n",
      "Epoch 32/50: 100%|██████████| 14/14 [00:05<00:00,  2.35it/s, loss=0.731]\n",
      "2025-05-12 11:30:15,465 - __main__ - INFO - Epoch 32/50, Train Loss: 0.7309\n",
      "2025-05-12 11:30:18,582 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_32.pth\n",
      "Epoch 33/50: 100%|██████████| 14/14 [00:05<00:00,  2.35it/s, loss=0.73]\n",
      "2025-05-12 11:30:24,556 - __main__ - INFO - Epoch 33/50, Train Loss: 0.7304\n",
      "2025-05-12 11:30:26,453 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_33.pth\n",
      "Epoch 34/50: 100%|██████████| 14/14 [00:05<00:00,  2.37it/s, loss=0.73]\n",
      "2025-05-12 11:30:32,363 - __main__ - INFO - Epoch 34/50, Train Loss: 0.7299\n",
      "2025-05-12 11:30:36,650 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_34.pth\n",
      "Epoch 35/50: 100%|██████████| 14/14 [00:05<00:00,  2.37it/s, loss=0.729]\n",
      "2025-05-12 11:30:42,574 - __main__ - INFO - Epoch 35/50, Train Loss: 0.7296\n",
      "2025-05-12 11:30:45,687 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_35.pth\n",
      "Epoch 36/50: 100%|██████████| 14/14 [00:05<00:00,  2.37it/s, loss=0.729]\n",
      "2025-05-12 11:30:51,598 - __main__ - INFO - Epoch 36/50, Train Loss: 0.7292\n",
      "2025-05-12 11:30:53,418 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_36.pth\n",
      "Epoch 37/50: 100%|██████████| 14/14 [00:05<00:00,  2.37it/s, loss=0.729]\n",
      "2025-05-12 11:30:59,329 - __main__ - INFO - Epoch 37/50, Train Loss: 0.7289\n",
      "2025-05-12 11:31:03,876 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_37.pth\n",
      "Epoch 38/50: 100%|██████████| 14/14 [00:05<00:00,  2.37it/s, loss=0.729]\n",
      "2025-05-12 11:31:09,793 - __main__ - INFO - Epoch 38/50, Train Loss: 0.7287\n",
      "2025-05-12 11:31:12,720 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_38.pth\n",
      "Epoch 39/50: 100%|██████████| 14/14 [00:05<00:00,  2.35it/s, loss=0.728]\n",
      "2025-05-12 11:31:18,690 - __main__ - INFO - Epoch 39/50, Train Loss: 0.7284\n",
      "2025-05-12 11:31:21,140 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_39.pth\n",
      "Epoch 40/50: 100%|██████████| 14/14 [00:05<00:00,  2.37it/s, loss=0.728]\n",
      "2025-05-12 11:31:27,064 - __main__ - INFO - Epoch 40/50, Train Loss: 0.7282\n",
      "2025-05-12 11:31:29,776 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_40.pth\n",
      "Epoch 41/50: 100%|██████████| 14/14 [00:05<00:00,  2.37it/s, loss=0.728]\n",
      "2025-05-12 11:31:35,682 - __main__ - INFO - Epoch 41/50, Train Loss: 0.7281\n",
      "2025-05-12 11:31:38,650 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_41.pth\n",
      "Epoch 42/50: 100%|██████████| 14/14 [00:05<00:00,  2.37it/s, loss=0.728]\n",
      "2025-05-12 11:31:44,566 - __main__ - INFO - Epoch 42/50, Train Loss: 0.7280\n",
      "2025-05-12 11:31:49,554 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_42.pth\n",
      "Epoch 43/50: 100%|██████████| 14/14 [00:05<00:00,  2.37it/s, loss=0.728]\n",
      "2025-05-12 11:31:55,469 - __main__ - INFO - Epoch 43/50, Train Loss: 0.7278\n",
      "2025-05-12 11:31:59,223 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_43.pth\n",
      "Epoch 44/50: 100%|██████████| 14/14 [00:05<00:00,  2.37it/s, loss=0.728]\n",
      "2025-05-12 11:32:05,125 - __main__ - INFO - Epoch 44/50, Train Loss: 0.7278\n",
      "2025-05-12 11:32:08,054 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_44.pth\n",
      "Epoch 45/50: 100%|██████████| 14/14 [00:05<00:00,  2.37it/s, loss=0.728]\n",
      "2025-05-12 11:32:13,958 - __main__ - INFO - Epoch 45/50, Train Loss: 0.7277\n",
      "2025-05-12 11:32:16,134 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_45.pth\n",
      "Epoch 46/50: 100%|██████████| 14/14 [00:05<00:00,  2.35it/s, loss=0.728]\n",
      "2025-05-12 11:32:22,108 - __main__ - INFO - Epoch 46/50, Train Loss: 0.7277\n",
      "2025-05-12 11:32:25,203 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_46.pth\n",
      "Epoch 47/50: 100%|██████████| 14/14 [00:05<00:00,  2.36it/s, loss=0.728]\n",
      "2025-05-12 11:32:31,142 - __main__ - INFO - Epoch 47/50, Train Loss: 0.7276\n",
      "2025-05-12 11:32:34,043 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_47.pth\n",
      "Epoch 48/50: 100%|██████████| 14/14 [00:05<00:00,  2.36it/s, loss=0.728]\n",
      "2025-05-12 11:32:39,992 - __main__ - INFO - Epoch 48/50, Train Loss: 0.7276\n",
      "2025-05-12 11:32:44,077 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_48.pth\n",
      "Epoch 49/50: 100%|██████████| 14/14 [00:05<00:00,  2.37it/s, loss=0.728]\n",
      "2025-05-12 11:32:49,985 - __main__ - INFO - Epoch 49/50, Train Loss: 0.7276\n",
      "2025-05-12 11:32:53,438 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_49.pth\n",
      "Epoch 50/50: 100%|██████████| 14/14 [00:05<00:00,  2.36it/s, loss=0.727]\n",
      "2025-05-12 11:32:59,362 - __main__ - INFO - Epoch 50/50, Train Loss: 0.7276\n",
      "2025-05-12 11:33:02,561 - __main__ - INFO - Checkpoint saved to checkpoints/bottle_epoch_50.pth\n",
      "2025-05-12 11:33:02,562 - __main__ - INFO - Evaluating model for category: bottle\n",
      "Evaluating bottle: 100%|██████████| 6/6 [00:40<00:00,  6.68s/it]\n",
      "2025-05-12 11:33:44,413 - __main__ - INFO - Pipeline completed in 8.53 minutes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_recall_curve\n",
    "import numpy as np\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "class Config:\n",
    "    dataset_path = \"./mvtec_anomaly_detection\"\n",
    "    categories = [\"bottle\"]\n",
    "    target_size = (224, 224)\n",
    "    batch_size = 16\n",
    "    num_epochs = 50\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 1e-5\n",
    "    clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "    embedding_dim = 512\n",
    "    margin = 0.5\n",
    "    temperature = 0.07\n",
    "    anomaly_threshold = 0.5\n",
    "    pixel_weight = 0.5\n",
    "    vis_save_dir = \"visualization_results\"\n",
    "    checkpoint_dir = \"checkpoints\"\n",
    "    use_fp16 = True\n",
    "    num_workers = 0\n",
    "\n",
    "    # Anomaly types\n",
    "    generic_anomaly_types = [\"scratched\", \"broken\", \"contaminated\", \"discolored\"]\n",
    "    specific_anomaly_types = {\n",
    "        \"bottle\": [\"dented\", \"leaking\", \"chipped\"],\n",
    "        \"cable\": [\"frayed\", \"kinked\"],\n",
    "        \"capsule\": [\"crushed\", \"color variation\"],\n",
    "        \"carpet\": [\"worn\", \"torn\"],\n",
    "        \"grid\": [\"distorted\", \"misaligned\"]\n",
    "    }\n",
    "\n",
    "class MVTecDataset(Dataset):\n",
    "    def __init__(self, root_dir, category, is_train=True, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.category = category\n",
    "        self.is_train = is_train\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.mask_paths = []\n",
    "        self.defect_types = []\n",
    "\n",
    "        if is_train:\n",
    "            train_good_path = os.path.join(root_dir, category, \"train\", \"good\")\n",
    "            self.image_paths = sorted(glob.glob(os.path.join(train_good_path, \"*.png\")))\n",
    "            self.labels = [0] * len(self.image_paths)\n",
    "            self.mask_paths = [None] * len(self.image_paths)\n",
    "            self.defect_types = [\"none\"] * len(self.image_paths)\n",
    "        else:\n",
    "            test_good_path = os.path.join(root_dir, category, \"test\", \"good\")\n",
    "            good_paths = sorted(glob.glob(os.path.join(test_good_path, \"*.png\")))\n",
    "            self.image_paths.extend(good_paths)\n",
    "            self.labels.extend([0] * len(good_paths))\n",
    "            self.defect_types.extend([\"none\"] * len(good_paths))\n",
    "            self.mask_paths.extend([None] * len(good_paths))\n",
    "\n",
    "            defect_types = [\n",
    "                d for d in os.listdir(os.path.join(root_dir, category, \"test\"))\n",
    "                if d != \"good\" and os.path.isdir(os.path.join(root_dir, category, \"test\", d))\n",
    "            ]\n",
    "            for defect_type in defect_types:\n",
    "                defect_path = os.path.join(root_dir, category, \"test\", defect_type)\n",
    "                defect_paths = sorted(glob.glob(os.path.join(defect_path, \"*.png\")))\n",
    "                self.image_paths.extend(defect_paths)\n",
    "                self.labels.extend([1] * len(defect_paths))\n",
    "                self.defect_types.extend([defect_type] * len(defect_paths))\n",
    "                for img_path in defect_paths:\n",
    "                    img_filename = os.path.basename(img_path)\n",
    "                    mask_path = os.path.join(\n",
    "                        root_dir, category, \"ground_truth\", defect_type, img_filename\n",
    "                    )\n",
    "                    mask_with_suffix = os.path.splitext(mask_path)[0] + \"_mask.png\"\n",
    "                    if os.path.exists(mask_path):\n",
    "                        self.mask_paths.append(mask_path)\n",
    "                    elif os.path.exists(mask_with_suffix):\n",
    "                        self.mask_paths.append(mask_with_suffix)\n",
    "                    else:\n",
    "                        self.mask_paths.append(None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        defect_type = self.defect_types[idx]\n",
    "        mask = None\n",
    "        if self.mask_paths[idx] and os.path.exists(self.mask_paths[idx]):\n",
    "            mask = Image.open(self.mask_paths[idx]).convert(\"L\")\n",
    "            mask = transforms.Resize(Config.target_size)(mask)\n",
    "            mask = transforms.ToTensor()(mask)\n",
    "        else:\n",
    "            mask = torch.zeros((1, *Config.target_size))  # Default empty mask\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"label\": label,\n",
    "            \"mask\": mask,\n",
    "            \"image_path\": image_path,\n",
    "            \"defect_type\": defect_type\n",
    "        }\n",
    "\n",
    "class VisionPromptLearner(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.embed_dim = clip_model.vision_model.config.hidden_size\n",
    "        self.prompt_length = 10  # Increased from 5 to 10\n",
    "        self.visual_prompts = nn.Parameter(torch.randn(self.prompt_length, self.embed_dim))\n",
    "\n",
    "    def forward(self, patch_embeddings):\n",
    "        prompts = self.visual_prompts.expand(patch_embeddings.shape[0], -1, -1)\n",
    "        return torch.cat([prompts, patch_embeddings], dim=1)\n",
    "\n",
    "class AnomalyLocalizationHead(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim * 2, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, patch_features, prompts):\n",
    "        batch_size, seq_len, embed_dim = prompts.shape\n",
    "        h = w = int(patch_features.shape[1]**0.5)\n",
    "\n",
    "        prompts_global = prompts.mean(dim=1)  # (batch_size, embed_dim)\n",
    "        prompts_expanded = prompts_global.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, h, w)\n",
    "        patch_reshaped = patch_features.view(batch_size, embed_dim, h, w)\n",
    "        fused = torch.cat([patch_reshaped, prompts_expanded], dim=1)\n",
    "        return self.fusion_layer(fused)\n",
    "\n",
    "class PromptBasedAnomalyDetector(nn.Module):\n",
    "    def __init__(self, clip_model_name=\"openai/clip-vit-base-patch32\"):\n",
    "        super().__init__()\n",
    "        self.clip_model = CLIPModel.from_pretrained(clip_model_name)\n",
    "        self.vision_encoder = self.clip_model.vision_model\n",
    "        self.vision_proj = self.clip_model.visual_projection\n",
    "        self.prompt_learner = VisionPromptLearner(self.clip_model)\n",
    "        self.anomaly_localization_head = AnomalyLocalizationHead(self.vision_encoder.config.hidden_size)\n",
    "\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        vision_outputs = self.vision_encoder(image)\n",
    "        image_embeddings = vision_outputs.last_hidden_state[:, 0, :]\n",
    "        image_embeddings = self.vision_proj(image_embeddings)\n",
    "        patch_embeddings = vision_outputs.last_hidden_state[:, 1:, :]\n",
    "        return image_embeddings, patch_embeddings\n",
    "\n",
    "    def forward(self, image, category):\n",
    "        image_embedding, patch_embeddings = self.encode_image(image)\n",
    "        patched_with_prompts = self.prompt_learner(patch_embeddings)\n",
    "        anomaly_map = self.anomaly_localization_head(patch_embeddings, patched_with_prompts)\n",
    "        anomaly_map = F.interpolate(anomaly_map, size=Config.target_size, mode='bilinear', align_corners=False)\n",
    "        anomaly_map = anomaly_map.squeeze(1)\n",
    "        anomaly_score = anomaly_map.mean(dim=(1, 2))\n",
    "        return {\n",
    "            'anomaly_score': anomaly_score,\n",
    "            'anomaly_map': anomaly_map,\n",
    "            'image_embedding': image_embedding\n",
    "        }\n",
    "\n",
    "def dice_loss(preds, targets, smooth=1e-6):\n",
    "    preds = preds.contiguous().view(-1)\n",
    "    targets = targets.contiguous().view(-1)\n",
    "    intersection = (preds * targets).sum()\n",
    "    union = preds.sum() + targets.sum()\n",
    "    return 1 - (2 * intersection + smooth) / (union + smooth)\n",
    "\n",
    "def contrastive_eam_loss(anomaly_scores, labels, anomaly_maps, masks):\n",
    "    logits = anomaly_scores.unsqueeze(1)\n",
    "    targets = labels.float().unsqueeze(1)\n",
    "    image_loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "\n",
    "    if masks is not None and masks.shape == anomaly_maps.shape:\n",
    "        if masks.dim() == 3:\n",
    "            masks = masks.unsqueeze(1)\n",
    "        elif masks.dim() == 4 and masks.shape[1] != 1:\n",
    "            masks = masks[:, 0].unsqueeze(1)\n",
    "\n",
    "        pixel_loss_bce = F.binary_cross_entropy_with_logits(anomaly_maps, masks.float())\n",
    "        pixel_loss_dice = dice_loss(torch.sigmoid(anomaly_maps), masks.float())\n",
    "        total_loss = image_loss + Config.pixel_weight * (pixel_loss_bce + pixel_loss_dice)\n",
    "    else:\n",
    "        total_loss = image_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def visualize_results(image, anomaly_map, mask, score, path, category, defect_type):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()\n",
    "    image = (image * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406])\n",
    "    image = np.clip(image, 0, 1)\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    defect_title = defect_type.capitalize() if defect_type != \"none\" else \"Normal\"\n",
    "    plt.title(f\"Image\\nDefect Type: {defect_title}\")\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(f\"Anomaly Map\\n(Score: {score:.2f})\")\n",
    "    plt.imshow(anomaly_map, cmap='jet', vmin=0, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Ground Truth Mask\")\n",
    "    if mask is not None and mask.sum() > 0:\n",
    "        plt.imshow(mask[0], cmap='gray')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, \"No Mask\", ha='center', va='center')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(Config.vis_save_dir, category)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    plt.savefig(os.path.join(save_path, os.path.basename(path)))\n",
    "    plt.close()\n",
    "\n",
    "def compute_metrics(image_scores, image_labels, pixel_scores, pixel_labels):\n",
    "    image_roc_auc = 0.0\n",
    "    if len(set(image_labels)) > 1:\n",
    "        image_roc_auc = roc_auc_score(image_labels, image_scores)\n",
    "\n",
    "    pixel_roc_auc = 0.0\n",
    "    pixel_f1 = 0.0\n",
    "    pixel_acc = 0.0\n",
    "    best_threshold = Config.anomaly_threshold\n",
    "\n",
    "    if len(pixel_scores) > 0 and len(pixel_labels) > 0:\n",
    "        all_pixel_scores = np.concatenate(pixel_scores)\n",
    "        all_pixel_labels = np.concatenate(pixel_labels)\n",
    "\n",
    "        if len(np.unique(all_pixel_labels)) < 2:\n",
    "            logger.warning(\"Only one class found in pixel labels. Skipping pixel-level AUC.\")\n",
    "        else:\n",
    "            try:\n",
    "                pixel_roc_auc = roc_auc_score(all_pixel_labels, all_pixel_scores)\n",
    "                precisions, recalls, thresholds = precision_recall_curve(all_pixel_labels, all_pixel_scores)\n",
    "                f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
    "                best_idx = np.argmax(f1_scores)\n",
    "                best_threshold = thresholds[best_idx]\n",
    "                pixel_preds = (all_pixel_scores > best_threshold).astype(int)\n",
    "                pixel_f1 = f1_score(all_pixel_labels, pixel_preds, zero_division=0)\n",
    "                pixel_acc = accuracy_score(all_pixel_labels, pixel_preds)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to compute metrics: {str(e)}\")\n",
    "\n",
    "    return {\n",
    "        'image_roc_auc': float(image_roc_auc),\n",
    "        'pixel_roc_auc': float(pixel_roc_auc),\n",
    "        'pixel_f1': float(pixel_f1),\n",
    "        'pixel_accuracy': float(pixel_acc),\n",
    "        'best_threshold': float(best_threshold)\n",
    "    }\n",
    "\n",
    "def train_model(model, category, train_loader):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=Config.learning_rate, weight_decay=Config.weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Config.num_epochs)\n",
    "    scaler = GradScaler() if Config.use_fp16 else None\n",
    "\n",
    "    augmentation = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2)\n",
    "    ])\n",
    "\n",
    "    for epoch in range(Config.num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Config.num_epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].float().to(device)\n",
    "            masks = batch['mask'].to(device)\n",
    "\n",
    "            augmented_images = augmentation(images)\n",
    "\n",
    "            with autocast(enabled=Config.use_fp16):\n",
    "                outputs = model(images, category)\n",
    "                aug_outputs = model(augmented_images, category)\n",
    "\n",
    "                loss = contrastive_eam_loss(\n",
    "                    outputs['anomaly_score'],\n",
    "                    labels,\n",
    "                    outputs['anomaly_map'],\n",
    "                    batch['mask'].to(device)\n",
    "                )\n",
    "                consistency_loss = F.mse_loss(outputs['anomaly_map'], aug_outputs['anomaly_map'])\n",
    "                total_loss = loss + 0.5 * consistency_loss\n",
    "\n",
    "            if Config.use_fp16:\n",
    "                scaler.scale(total_loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_loss += total_loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": total_loss.item()})\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        logger.info(f\"Epoch {epoch+1}/{Config.num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "        scheduler.step()\n",
    "        save_checkpoint(model, optimizer, epoch, avg_train_loss, f\"{Config.checkpoint_dir}/{category}_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, category, test_loader):\n",
    "    model.eval()\n",
    "    image_scores = []\n",
    "    image_labels = []\n",
    "    pixel_scores = []\n",
    "    pixel_labels = []\n",
    "    defect_specific_scores = {}\n",
    "\n",
    "    os.makedirs(os.path.join(Config.vis_save_dir, category), exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=f\"Evaluating {category}\"):\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].cpu().numpy()\n",
    "            masks = batch['mask'].cpu().numpy()\n",
    "            image_paths = batch['image_path']\n",
    "            defect_types = batch['defect_type']\n",
    "\n",
    "            outputs = model(images, category)\n",
    "            anomaly_scores = outputs['anomaly_score'].cpu().numpy()\n",
    "            anomaly_maps = outputs['anomaly_map'].cpu().numpy()\n",
    "\n",
    "            image_scores.extend(anomaly_scores)\n",
    "            image_labels.extend(labels)\n",
    "\n",
    "            for idx, defect_type in enumerate(defect_types):\n",
    "                if defect_type not in defect_specific_scores:\n",
    "                    defect_specific_scores[defect_type] = {\"scores\": [], \"labels\": []}\n",
    "                defect_specific_scores[defect_type][\"scores\"].append(anomaly_scores[idx])\n",
    "                defect_specific_scores[defect_type][\"labels\"].append(labels[idx])\n",
    "\n",
    "            for b in range(images.shape[0]):\n",
    "                if labels[b] == 1 and masks[b].sum() > 0:\n",
    "                    pixel_scores.append(anomaly_maps[b].flatten())\n",
    "                    pixel_labels.append(masks[b].flatten().astype(int))\n",
    "                visualize_results(images[b], anomaly_maps[b], masks[b], anomaly_scores[b], image_paths[b], category, defect_types[b])\n",
    "\n",
    "    metrics = compute_metrics(image_scores, image_labels, pixel_scores, pixel_labels)\n",
    "    metrics['category'] = category\n",
    "\n",
    "    defect_specific_aucs = {}\n",
    "    for defect_type, data in defect_specific_scores.items():\n",
    "        if defect_type != \"none\" and len(set(data[\"labels\"])) > 1:\n",
    "            try:\n",
    "                defect_specific_aucs[defect_type] = roc_auc_score(data[\"labels\"], data[\"scores\"])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    metrics['defect_specific_aucs'] = {k: float(v) for k, v in defect_specific_aucs.items()}\n",
    "    with open(os.path.join(Config.vis_save_dir, f\"{category}_metrics.json\"), 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filename):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'loss': loss\n",
    "    }, filename)\n",
    "    logger.info(f\"Checkpoint saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    logger.info(\"Starting improved anomaly detection pipeline...\")\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "        logger.warning(\"CUDA is not available. Using CPU, which will be much slower!\")\n",
    "\n",
    "    all_metrics = {}\n",
    "    os.makedirs(Config.vis_save_dir, exist_ok=True)\n",
    "    os.makedirs(Config.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    for category in Config.categories:\n",
    "        logger.info(f\"Processing category: {category}\")\n",
    "\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize(Config.target_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        test_transform = transforms.Compose([\n",
    "            transforms.Resize(Config.target_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        train_dataset = MVTecDataset(Config.dataset_path, category, is_train=True, transform=train_transform)\n",
    "        test_dataset = MVTecDataset(Config.dataset_path, category, is_train=False, transform=test_transform)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=Config.batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "        model = PromptBasedAnomalyDetector(Config.clip_model_name).to(device)\n",
    "\n",
    "        logger.info(f\"Training model for category: {category}\")\n",
    "        model = train_model(model, category, train_loader)\n",
    "\n",
    "        logger.info(f\"Evaluating model for category: {category}\")\n",
    "        metrics = evaluate_model(model, category, test_loader)\n",
    "        all_metrics[category] = metrics\n",
    "\n",
    "    overall_metrics = {\n",
    "        \"overall_avg_image_auc\": sum(m['image_roc_auc'] for m in all_metrics.values()) / len(all_metrics),\n",
    "        \"overall_avg_pixel_auc\": sum(m['pixel_roc_auc'] for m in all_metrics.values()) / len(all_metrics),\n",
    "        \"overall_avg_pixel_f1\": sum(m['pixel_f1'] for m in all_metrics.values()) / len(all_metrics),\n",
    "        \"categories\": list(all_metrics.keys()),\n",
    "        \"category_performance\": {\n",
    "            cat: {\n",
    "                \"image_auc\": m[\"image_roc_auc\"],\n",
    "                \"pixel_auc\": m[\"pixel_roc_auc\"],\n",
    "                \"pixel_f1\": m[\"pixel_f1\"]\n",
    "            } for cat, m in all_metrics.items()\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(Config.vis_save_dir, \"overall_metrics.json\"), 'w') as f:\n",
    "        json.dump(overall_metrics, f, indent=4)\n",
    "\n",
    "    elapsed_time = (time.time() - start_time) / 60.0\n",
    "    logger.info(f\"Pipeline completed in {elapsed_time:.2f} minutes.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c812b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 12:01:18,534 - __main__ - INFO - Using device: cuda\n",
      "2025-05-12 12:01:18,539 - __main__ - INFO - Starting improved anomaly detection pipeline...\n",
      "2025-05-12 12:01:18,540 - __main__ - INFO - Processing category: bottle\n",
      "2025-05-12 12:01:30,294 - __main__ - INFO - Training model for category: bottle\n",
      "Epoch 1/30:   0%|          | 0/14 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, got input (16), mat (16x512), vec (768)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 526\u001b[0m\n\u001b[0;32m    523\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 526\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[45], line 507\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    505\u001b[0m model \u001b[38;5;241m=\u001b[39m PromptBasedAnomalyDetector(Config\u001b[38;5;241m.\u001b[39mclip_model_name)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    506\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model for category: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 507\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating model for category: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m metrics \u001b[38;5;241m=\u001b[39m evaluate_model(model, category, test_loader)\n",
      "Cell \u001b[1;32mIn[45], line 369\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, category, train_loader)\u001b[0m\n\u001b[0;32m    366\u001b[0m     masks \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    367\u001b[0m pixel_loss \u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39mpixel_weight \u001b[38;5;241m*\u001b[39m dice_loss(outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manomaly_map\u001b[39m\u001b[38;5;124m'\u001b[39m], masks\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m--> 369\u001b[0m contrastive_loss \u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39mlambda_prompt \u001b[38;5;241m*\u001b[39m \u001b[43mcontrastive_prompt_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manomaly_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_embedding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    376\u001b[0m eam \u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39mlambda_eam \u001b[38;5;241m*\u001b[39m eam_loss(\n\u001b[0;32m    377\u001b[0m     model\u001b[38;5;241m.\u001b[39mprompt_learner\u001b[38;5;241m.\u001b[39mnormal_prompts,\n\u001b[0;32m    378\u001b[0m     model\u001b[38;5;241m.\u001b[39mprompt_learner\u001b[38;5;241m.\u001b[39manomaly_prompts\n\u001b[0;32m    379\u001b[0m )\n\u001b[0;32m    381\u001b[0m alignment \u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39mlambda_align \u001b[38;5;241m*\u001b[39m prompt_alignment_loss(\n\u001b[0;32m    382\u001b[0m     model\u001b[38;5;241m.\u001b[39mprompt_learner\u001b[38;5;241m.\u001b[39manomaly_prompts,\n\u001b[0;32m    383\u001b[0m     text_embeddings\n\u001b[0;32m    384\u001b[0m )\n",
      "Cell \u001b[1;32mIn[45], line 215\u001b[0m, in \u001b[0;36mcontrastive_prompt_loss\u001b[1;34m(normal_prompts, anomaly_prompts, cls_embeddings, labels)\u001b[0m\n\u001b[0;32m    212\u001b[0m normal_proto \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(normal_prompts\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    213\u001b[0m anomaly_proto \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(anomaly_prompts\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 215\u001b[0m sim_to_normal \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormal_proto\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m sim_to_anomaly \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(cls_embeddings, anomaly_proto)\n\u001b[0;32m    218\u001b[0m pos_loss \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m sim_to_normal[labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, got input (16), mat (16x512), vec (768)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, precision_recall_curve\n",
    "import numpy as np\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "class Config:\n",
    "    dataset_path = \"./data\"\n",
    "    categories = [\"bottle\"]\n",
    "    target_size = (224, 224)\n",
    "    batch_size = 16\n",
    "    num_epochs = 30\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 1e-5\n",
    "    clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "    embedding_dim = 768\n",
    "    margin = 0.5\n",
    "    temperature = 0.07\n",
    "    anomaly_threshold = 0.5\n",
    "    pixel_weight = 0.5\n",
    "    vis_save_dir = \"visualization_results\"\n",
    "    checkpoint_dir = \"checkpoints\"\n",
    "    use_fp16 = True\n",
    "    num_workers = 0\n",
    "\n",
    "    # Hyperparameters\n",
    "    lambda_prompt = 0.5\n",
    "    lambda_eam = 0.3\n",
    "    lambda_align = 0.2\n",
    "    prompt_learning_rate = 1e-4\n",
    "\n",
    "    # Anomaly types\n",
    "    generic_anomaly_types = [\"scratched\", \"broken\", \"contaminated\", \"discolored\"]\n",
    "    specific_anomaly_types = {\n",
    "        \"bottle\": [\"dented\", \"leaking\", \"chipped\"],\n",
    "        \"cable\": [\"frayed\", \"kinked\"],\n",
    "        \"capsule\": [\"crushed\", \"color variation\"],\n",
    "        \"carpet\": [\"worn\", \"torn\"],\n",
    "        \"grid\": [\"distorted\", \"misaligned\"]\n",
    "    }\n",
    "\n",
    "class MVTecDataset(Dataset):\n",
    "    def __init__(self, root_dir, category, is_train=True, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.category = category\n",
    "        self.is_train = is_train\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.mask_paths = []\n",
    "        self.defect_types = []\n",
    "\n",
    "        if is_train:\n",
    "            train_good_path = os.path.join(root_dir, category, \"train\", \"good\")\n",
    "            self.image_paths = sorted(glob.glob(os.path.join(train_good_path, \"*.png\")))\n",
    "            self.labels = [0] * len(self.image_paths)\n",
    "            self.mask_paths = [None] * len(self.image_paths)\n",
    "            self.defect_types = [\"none\"] * len(self.image_paths)\n",
    "        else:\n",
    "            test_good_path = os.path.join(root_dir, category, \"test\", \"good\")\n",
    "            good_paths = sorted(glob.glob(os.path.join(test_good_path, \"*.png\")))\n",
    "            self.image_paths.extend(good_paths)\n",
    "            self.labels.extend([0] * len(good_paths))\n",
    "            self.defect_types.extend([\"none\"] * len(good_paths))\n",
    "            self.mask_paths.extend([None] * len(good_paths))\n",
    "\n",
    "            defect_types = [\n",
    "                d for d in os.listdir(os.path.join(root_dir, category, \"test\"))\n",
    "                if d != \"good\" and os.path.isdir(os.path.join(root_dir, category, \"test\", d))\n",
    "            ]\n",
    "            for defect_type in defect_types:\n",
    "                defect_path = os.path.join(root_dir, category, \"test\", defect_type)\n",
    "                defect_paths = sorted(glob.glob(os.path.join(defect_path, \"*.png\")))\n",
    "                self.image_paths.extend(defect_paths)\n",
    "                self.labels.extend([1] * len(defect_paths))\n",
    "                self.defect_types.extend([defect_type] * len(defect_paths))\n",
    "                for img_path in defect_paths:\n",
    "                    img_filename = os.path.basename(img_path)\n",
    "                    mask_path = os.path.join(\n",
    "                        root_dir, category, \"ground_truth\", defect_type, img_filename\n",
    "                    )\n",
    "                    mask_with_suffix = os.path.splitext(mask_path)[0] + \"_mask.png\"\n",
    "                    if os.path.exists(mask_path):\n",
    "                        self.mask_paths.append(mask_path)\n",
    "                    elif os.path.exists(mask_with_suffix):\n",
    "                        self.mask_paths.append(mask_with_suffix)\n",
    "                    else:\n",
    "                        self.mask_paths.append(None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        label = self.labels[idx]\n",
    "        defect_type = self.defect_types[idx]\n",
    "\n",
    "        mask = None\n",
    "        if self.mask_paths[idx] and os.path.exists(self.mask_paths[idx]):\n",
    "            mask = Image.open(self.mask_paths[idx]).convert(\"L\")\n",
    "            mask = transforms.Resize(Config.target_size)(mask)\n",
    "            mask = transforms.ToTensor()(mask)\n",
    "        else:\n",
    "            mask = torch.zeros((1, *Config.target_size))  # Default empty mask\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"label\": label,\n",
    "            \"mask\": mask,\n",
    "            \"image_path\": image_path,\n",
    "            \"defect_type\": defect_type\n",
    "        }\n",
    "\n",
    "class VisionPromptLearner(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.embed_dim = clip_model.vision_model.config.hidden_size\n",
    "        self.prompt_length = 5\n",
    "        self.normal_prompts = nn.Parameter(torch.randn(self.prompt_length, self.embed_dim))\n",
    "        self.anomaly_prompts = nn.Parameter(torch.randn(self.prompt_length, self.embed_dim))\n",
    "\n",
    "    def forward(self, patch_embeddings):\n",
    "        prompts = torch.cat([self.normal_prompts, self.anomaly_prompts], dim=0)\n",
    "        prompts = prompts.expand(patch_embeddings.shape[0], -1, -1)\n",
    "        return torch.cat([prompts, patch_embeddings], dim=1)\n",
    "\n",
    "class AnomalyLocalizationHead(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim * 2, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 1, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, patch_features, prompts):\n",
    "        batch_size, seq_len, embed_dim = prompts.shape\n",
    "        h = w = int(patch_features.shape[1]**0.5)\n",
    "        prompts_global = prompts.mean(dim=1)\n",
    "        prompts_expanded = prompts_global.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, h, w)\n",
    "        patch_reshaped = patch_features.view(batch_size, embed_dim, h, w)\n",
    "        fused = torch.cat([patch_reshaped, prompts_expanded], dim=1)\n",
    "        return self.fusion_layer(fused)\n",
    "\n",
    "class PromptBasedAnomalyDetector(nn.Module):\n",
    "    def __init__(self, clip_model_name=\"openai/clip-vit-base-patch32\"):\n",
    "        super().__init__()\n",
    "        self.clip_model = CLIPModel.from_pretrained(clip_model_name)\n",
    "        self.vision_encoder = self.clip_model.vision_model\n",
    "        self.vision_proj = self.clip_model.visual_projection\n",
    "        self.prompt_learner = VisionPromptLearner(self.clip_model)\n",
    "        self.anomaly_localization_head = AnomalyLocalizationHead(self.vision_encoder.config.hidden_size)\n",
    "\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        vision_outputs = self.vision_encoder(image)\n",
    "        image_embeddings = vision_outputs.last_hidden_state[:, 0, :]\n",
    "        image_embeddings = self.vision_proj(image_embeddings)\n",
    "        patch_embeddings = vision_outputs.last_hidden_state[:, 1:, :]\n",
    "        return image_embeddings, patch_embeddings\n",
    "\n",
    "    def forward(self, image, category):\n",
    "        image_embedding, patch_embeddings = self.encode_image(image)\n",
    "        patched_with_prompts = self.prompt_learner(patch_embeddings)\n",
    "        anomaly_map = self.anomaly_localization_head(patch_embeddings, patched_with_prompts)\n",
    "        anomaly_map = F.interpolate(anomaly_map, size=Config.target_size, mode='bilinear', align_corners=False)\n",
    "        anomaly_map = anomaly_map.squeeze(1)\n",
    "        normalized_anomaly_map = torch.sigmoid(anomaly_map)\n",
    "        anomaly_score = normalized_anomaly_map.mean(dim=(1, 2))\n",
    "\n",
    "        return {\n",
    "            'anomaly_score': anomaly_score,\n",
    "            'anomaly_map': normalized_anomaly_map,\n",
    "            'image_embedding': image_embedding,\n",
    "            'patch_embeddings': patch_embeddings\n",
    "        }\n",
    "\n",
    "def contrastive_prompt_loss(normal_prompts, anomaly_prompts, cls_embeddings, labels):\n",
    "    cls_embeddings = F.normalize(cls_embeddings, dim=1)\n",
    "    normal_proto = F.normalize(normal_prompts.mean(dim=0), dim=0)\n",
    "    anomaly_proto = F.normalize(anomaly_prompts.mean(dim=0), dim=0)\n",
    "\n",
    "    sim_to_normal = torch.matmul(cls_embeddings, normal_proto)\n",
    "    sim_to_anomaly = torch.matmul(cls_embeddings, anomaly_proto)\n",
    "\n",
    "    pos_loss = (1 - sim_to_normal[labels == 0]).mean()\n",
    "    neg_loss = (1 + sim_to_anomaly[labels == 1]).mean()\n",
    "\n",
    "    return pos_loss + neg_loss\n",
    "\n",
    "def eam_loss(normal_prompts, anomaly_prompts, margin=Config.margin):\n",
    "    normal_proto = normal_prompts.mean(dim=0)\n",
    "    anomaly_proto = anomaly_prompts.mean(dim=0)\n",
    "    distance = torch.norm(normal_proto - anomaly_proto, p=2)\n",
    "    return torch.relu(margin - distance)\n",
    "\n",
    "def prompt_alignment_loss(learned_prompts, text_embeddings):\n",
    "    learned_prompts = F.normalize(learned_prompts, dim=1)\n",
    "    text_embeddings = F.normalize(text_embeddings, dim=1)\n",
    "    cos_sim = torch.mm(learned_prompts, text_embeddings.T)\n",
    "    return (1 - torch.diag(cos_sim)).mean()\n",
    "\n",
    "def dice_loss(preds, targets, smooth=1e-6):\n",
    "    preds = preds.contiguous().view(-1)\n",
    "    targets = targets.contiguous().view(-1)\n",
    "    intersection = (preds * targets).sum()\n",
    "    return 1 - (2. * intersection + smooth) / (preds.sum() + targets.sum() + smooth)\n",
    "\n",
    "def compute_vad_map(patch_embeddings):\n",
    "    prototype = patch_embeddings.mean(dim=0)\n",
    "    distances = torch.cdist(patch_embeddings, prototype.unsqueeze(0))\n",
    "    return distances.squeeze(-1)\n",
    "\n",
    "def compute_optimal_threshold(image_scores, image_labels):\n",
    "    precision, recall, thresholds = precision_recall_curve(image_labels, image_scores)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    return thresholds[best_idx]\n",
    "\n",
    "def visualize_results(image, anomaly_map, mask, score, path, category, defect_type):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()\n",
    "    image = (image * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406])\n",
    "    image = np.clip(image, 0, 1)\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    defect_title = defect_type.capitalize() if defect_type != \"none\" else \"Normal\"\n",
    "    plt.title(f\"Image\\nDefect Type: {defect_title}\")\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(f\"Anomaly Map\\n(Score: {score:.2f})\")\n",
    "    plt.imshow(anomaly_map, cmap='jet', vmin=0, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Ground Truth Mask\")\n",
    "    if mask is not None and mask.sum() > 0:\n",
    "        plt.imshow(mask[0], cmap='gray')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, \"No Mask\", ha='center', va='center')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(Config.vis_save_dir, category)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    plt.savefig(os.path.join(save_path, os.path.basename(path)))\n",
    "    plt.close()\n",
    "\n",
    "def compute_metrics(image_scores, image_labels, pixel_scores, pixel_labels):\n",
    "    image_roc_auc = 0.0\n",
    "    if len(set(image_labels)) > 1:\n",
    "        image_roc_auc = roc_auc_score(image_labels, image_scores)\n",
    "\n",
    "    pixel_roc_auc = 0.0\n",
    "    pixel_f1 = 0.0\n",
    "    pixel_acc = 0.0\n",
    "\n",
    "    if len(pixel_scores) > 0 and len(pixel_labels) > 0:\n",
    "        all_pixel_scores = np.concatenate(pixel_scores)\n",
    "        all_pixel_labels = np.concatenate(pixel_labels)\n",
    "\n",
    "        if len(np.unique(all_pixel_labels)) < 2:\n",
    "            logger.warning(\"Only one class found in pixel labels. Skipping pixel-level AUC.\")\n",
    "        else:\n",
    "            all_pixel_scores = 1 / (1 + np.exp(-all_pixel_scores))\n",
    "            try:\n",
    "                pixel_roc_auc = roc_auc_score(all_pixel_labels, all_pixel_scores)\n",
    "            except ValueError as e:\n",
    "                logger.error(f\"Failed to compute pixel AUC: {str(e)}\")\n",
    "\n",
    "        pixel_preds = (all_pixel_scores > Config.anomaly_threshold).astype(int)\n",
    "        pixel_f1 = f1_score(all_pixel_labels, pixel_preds, zero_division=0)\n",
    "        pixel_acc = accuracy_score(all_pixel_labels, pixel_preds)\n",
    "\n",
    "    return {\n",
    "        'image_roc_auc': float(image_roc_auc),\n",
    "        'pixel_roc_auc': float(pixel_roc_auc),\n",
    "        'pixel_f1': float(pixel_f1),\n",
    "        'pixel_accuracy': float(pixel_acc)\n",
    "    }\n",
    "\n",
    "def train_model(model, category, train_loader):\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': model.prompt_learner.normal_prompts, 'lr': Config.prompt_learning_rate},\n",
    "        {'params': model.prompt_learner.anomaly_prompts, 'lr': Config.prompt_learning_rate},\n",
    "        {'params': model.anomaly_localization_head.parameters(), 'lr': Config.learning_rate}\n",
    "    ], weight_decay=Config.weight_decay)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Config.num_epochs)\n",
    "    scaler = GradScaler() if Config.use_fp16 else None\n",
    "\n",
    "    augmentation = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1)\n",
    "    ])\n",
    "\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(Config.clip_model_name)\n",
    "    text_encoder = CLIPModel.from_pretrained(Config.clip_model_name).text_model.to(device)\n",
    "\n",
    "    def get_text_embeddings(texts):\n",
    "        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        text_features = text_encoder(**inputs).last_hidden_state[:, 0, :]\n",
    "        return F.normalize(text_features, dim=-1)\n",
    "\n",
    "    anomaly_texts = Config.specific_anomaly_types.get(category, [])\n",
    "    if not anomaly_texts:\n",
    "        anomaly_texts = Config.generic_anomaly_types[:3]\n",
    "\n",
    "    text_embeddings = get_text_embeddings(anomaly_texts)\n",
    "\n",
    "    for epoch in range(Config.num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Config.num_epochs}\")\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].float().to(device)\n",
    "            masks = batch['mask'].to(device)\n",
    "\n",
    "            augmented_images = augmentation(images)\n",
    "\n",
    "            with autocast(enabled=Config.use_fp16):\n",
    "                outputs = model(images, category)\n",
    "                aug_outputs = model(augmented_images, category)\n",
    "\n",
    "                # Compute losses\n",
    "                bce_loss = F.binary_cross_entropy_with_logits(outputs['anomaly_score'], labels)\n",
    "                if masks.dim() == 3:\n",
    "                    masks = masks.unsqueeze(1)\n",
    "                pixel_loss = Config.pixel_weight * dice_loss(outputs['anomaly_map'], masks.float())\n",
    "\n",
    "                contrastive_loss = Config.lambda_prompt * contrastive_prompt_loss(\n",
    "                    model.prompt_learner.normal_prompts,\n",
    "                    model.prompt_learner.anomaly_prompts,\n",
    "                    outputs['image_embedding'],\n",
    "                    labels.long()\n",
    "                )\n",
    "\n",
    "                eam = Config.lambda_eam * eam_loss(\n",
    "                    model.prompt_learner.normal_prompts,\n",
    "                    model.prompt_learner.anomaly_prompts\n",
    "                )\n",
    "\n",
    "                alignment = Config.lambda_align * prompt_alignment_loss(\n",
    "                    model.prompt_learner.anomaly_prompts,\n",
    "                    text_embeddings\n",
    "                )\n",
    "\n",
    "                consistency_loss = F.mse_loss(outputs['anomaly_map'], aug_outputs['anomaly_map'])\n",
    "\n",
    "                total_loss = bce_loss + pixel_loss + contrastive_loss + eam + alignment + 0.5 * consistency_loss\n",
    "\n",
    "            if Config.use_fp16:\n",
    "                scaler.scale(total_loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss += total_loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": total_loss.item()})\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        logger.info(f\"Epoch {epoch+1}/{Config.num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "        scheduler.step()\n",
    "\n",
    "        save_checkpoint(model, optimizer, epoch, avg_train_loss, f\"{Config.checkpoint_dir}/{category}_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, category, test_loader):\n",
    "    model.eval()\n",
    "    image_scores = []\n",
    "    image_labels = []\n",
    "    pixel_scores = []\n",
    "    pixel_labels = []\n",
    "    defect_specific_scores = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=f\"Evaluating {category}\"):\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].cpu().numpy()\n",
    "            masks = batch['mask'].cpu().numpy()\n",
    "            image_paths = batch['image_path']\n",
    "            defect_types = batch['defect_type']\n",
    "\n",
    "            outputs = model(images, category)\n",
    "            anomaly_scores = outputs['anomaly_score'].cpu().numpy()\n",
    "            anomaly_maps = outputs['anomaly_map'].cpu().numpy()\n",
    "\n",
    "            image_scores.extend(anomaly_scores)\n",
    "            image_labels.extend(labels)\n",
    "\n",
    "            for idx, defect_type in enumerate(defect_types):\n",
    "                if defect_type not in defect_specific_scores:\n",
    "                    defect_specific_scores[defect_type] = {\"scores\": [], \"labels\": []}\n",
    "                defect_specific_scores[defect_type][\"scores\"].append(anomaly_scores[idx])\n",
    "                defect_specific_scores[defect_type][\"labels\"].append(labels[idx])\n",
    "\n",
    "            for b in range(images.shape[0]):\n",
    "                if labels[b] == 1 and masks[b].sum() > 0:\n",
    "                    pixel_scores.append(anomaly_maps[b].flatten())\n",
    "                    pixel_labels.append(masks[b].flatten().astype(int))\n",
    "                visualize_results(images[b], anomaly_maps[b], masks[b], anomaly_scores[b], image_paths[b], category, defect_types[b])\n",
    "\n",
    "    optimal_threshold = compute_optimal_threshold(image_scores, image_labels)\n",
    "    Config.anomaly_threshold = optimal_threshold\n",
    "\n",
    "    metrics = compute_metrics(image_scores, image_labels, pixel_scores, pixel_labels)\n",
    "    metrics['category'] = category\n",
    "    metrics['optimal_threshold'] = float(optimal_threshold)\n",
    "\n",
    "    defect_specific_aucs = {}\n",
    "    for defect_type, data in defect_specific_scores.items():\n",
    "        if defect_type != \"none\" and len(set(data[\"labels\"])) > 1:\n",
    "            try:\n",
    "                defect_specific_aucs[defect_type] = roc_auc_score(data[\"labels\"], data[\"scores\"])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    metrics['defect_specific_aucs'] = {k: float(v) for k, v in defect_specific_aucs.items()}\n",
    "    with open(os.path.join(Config.vis_save_dir, f\"{category}_metrics.json\"), 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, loss, filename):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'loss': loss\n",
    "    }, filename)\n",
    "    logger.info(f\"Checkpoint saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    logger.info(\"Starting improved anomaly detection pipeline...\")\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "        logger.warning(\"CUDA is not available. Using CPU, which will be much slower!\")\n",
    "\n",
    "    all_metrics = {}\n",
    "    os.makedirs(Config.vis_save_dir, exist_ok=True)\n",
    "    os.makedirs(Config.checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    for category in Config.categories:\n",
    "        logger.info(f\"Processing category: {category}\")\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize(Config.target_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        test_transform = transforms.Compose([\n",
    "            transforms.Resize(Config.target_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        train_dataset = MVTecDataset(Config.dataset_path, category, is_train=True, transform=train_transform)\n",
    "        test_dataset = MVTecDataset(Config.dataset_path, category, is_train=False, transform=test_transform)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=Config.batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "        model = PromptBasedAnomalyDetector(Config.clip_model_name).to(device)\n",
    "        logger.info(f\"Training model for category: {category}\")\n",
    "        model = train_model(model, category, train_loader)\n",
    "        logger.info(f\"Evaluating model for category: {category}\")\n",
    "        metrics = evaluate_model(model, category, test_loader)\n",
    "        all_metrics[category] = metrics\n",
    "\n",
    "    overall_metrics = {\n",
    "        \"overall_avg_image_auc\": sum(m['image_roc_auc'] for m in all_metrics.values()) / len(all_metrics),\n",
    "        \"overall_avg_pixel_auc\": sum(m['pixel_roc_auc'] for m in all_metrics.values()) / len(all_metrics),\n",
    "        \"categories\": list(all_metrics.keys()),\n",
    "        \"category_performance\": {cat: {\"image_auc\": m[\"image_roc_auc\"], \"pixel_auc\": m[\"pixel_roc_auc\"]} for cat, m in all_metrics.items()}\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(Config.vis_save_dir, \"overall_metrics.json\"), 'w') as f:\n",
    "        json.dump(overall_metrics, f, indent=4)\n",
    "\n",
    "    elapsed_time = (time.time() - start_time) / 60.0\n",
    "    logger.info(f\"Pipeline completed in {elapsed_time:.2f} minutes.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RTX3060ti_Pytorch",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
